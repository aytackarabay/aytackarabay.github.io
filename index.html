<!DOCTYPE html>
<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Aytac Karabay</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="owwwlab.com">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        
        <meta name="description" content="Aytac Karabay personal webpage" />
        <meta name="keywords" content="faculty profile, theme,css, html, jquery, transition, transform, 3d, css3" />

        <link rel="shortcut icon" href="favicon.ico">

        <!--CSS styles-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">  
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/magnific-popup.css">
        <link rel="stylesheet" href="css/style.css">
        <link id="theme-style" rel="stylesheet" href="css/theme-style.css">
		<link href="https://fonts.googleapis.com/css?family=Open+Sans&amp;subset=latin-ext" rel="stylesheet">
        
        <!--/CSS styles-->
        <!--Javascript files-->
        <script type="text/javascript" src="js/jquery-1.10.2.js"></script>
        <script type="text/javascript" src="js/TweenMax.min.js"></script>
        <script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
        <script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>
        
        <script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="js/jquery.dropdownit.js"></script>

        <script type="text/javascript" src="js/jquery.stellar.min.js"></script>
        <script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

        <script type="text/javascript" src="js/bootstrap.min.js"></script>

        <script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

        <script type="text/javascript" src="js/masonry.min.js"></script>

        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>

        <script type="text/javascript" src="js/magnific-popup.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <!--/Javascript files-->

    </head>
    <body>
		
        <div id="wrapper">
            <a href="#sidebar" class="mobilemenu"><i class="icon-reorder"></i></a>

            <div id="sidebar">
                <div id="main-nav">
                    <div id="nav-container">
                        <div id="profile" class="clearfix">
                            <div class="portrate hidden-xs"></div>
                            <!--<div class="title">
                                <h2>Aytaç Karabay</h2>
                                <h3>University of Groningen</h3>
                            </div>-->
                            
                        </div>
                        <ul id="navigation">
                            <li>
                              <a href="#biography">
                                <div class="icon icon-user"></div>
                                <div class="text">About Me</div>
                              </a>
                            </li>  
                            
                            <li>
                              <a href="#research">
                                <div class="icon icon-book"></div>
                                <div class="text">Blog</div>
                              </a>
                            </li> 
                            
                            <li>
                              <a href="#publications">
                                <div class="icon icon-edit"></div>
                                <div class="text">Publications</div>
                              </a>
                            </li> 
							
                            <li>
                              <a href="#experiments">
                                <div class="icon icon-bullseye"></div>
                                <div class="text">Tasks</div>
                              </a>
                            </li> 
                            <!--<li>
                              <a href="#teaching">
                                <div class="icon icon-time"></div>
                                <div class="text">Teaching</div>
                              </a>
                            </li>-->

                            <!--<li>
                              <a href="#gallery">
                                <div class="icon icon-picture"></div>
                                <div class="text">Gallery</div>
                              </a>
                            </li>-->

                            <li class="external">
                              <a target="_blank" href="#">
                                  <div class="icon icon-download-alt"></div>
                                  <div class="text"><a href="Files/download/Resume_Karabay.pdf" target="_blank">CV</a></div>
                              </a>
                            </li>
							
                            <li>
                              <a href="#contact">
                                  <div class="icon icon-calendar"></div>
                                  <div class="text">Contact & Collaborate</div>
                              </a>
                            </li>

                        </ul>
                    </div>        
                </div>
                
                <div class="social-icons">
                    <!--<ul>
                        <li><a href="#"><i class="icon-facebook"></i></a></li>
                        <li><a href="#"><i class="icon-twitter"></i></a></li>
                        <li><a href="#"><i class="icon-linkedin"></i></a></li>
                    </ul>-->
                </div>    
            </div>

            <div id="main">
		        <div id="biography" class="page home" data-pos="home">
							<div class="row2">
								<div id="rightlinks">
									<a href="indexTR.html">TR</a> 
									<a href="index.html">EN</a> 
								</div>	
							</div>
							   
					<!--<div class="section color-2">
                        <div class="section-container2">
                                 
                        </div>
                    </div>-->
					
                    <div class="pageheader">
                        <div class="headercontent">
                            <div class="section-container">
								<div class="row">
                                    <div class="col-sm-2 visible-sm"></div>
                                    
                                    <div class="clearfix visible-sm visible-xs"></div>
                                    <div class="col-sm-12 col-md-10">
                                        <h3 class="title">Bio</h3>
										<div class="col-sm-8 col-md-5">
                                        <div class="biothumb">
                                            <img alt="image" src="img/personal/personal-big.jpg" class="img-responsive">
                                            <div class="overlay">
											                                                
                                                <h3 class="">Aytaç Karabay, PhD</h3>
                                                <ul class="list-unstyled">
                                                    <li>Cognitive Science</li>
                                                    <li>New York University Abu Dhabi</li>
                                                </ul>
                                            </div> 
                                        </div>
										</div>
										I, <b>Aytaç Karabay (1989)</b>, am a postdoctoral researcher at the New York University Abu Dhabi. I studied psychology (BA) 
										from 2007 to 2011 at Ankara University. After completing my BA studies, I worked in different positions as a psychologist
										for a year. Then, I started my master's in experimental psychology at the Brooklyn College-City University of New York in
										2013. I worked in <a href="https://www.gc.cuny.edu/people/daniel-kurylo" target="_blank">Daniel Kurylo</a>'s animal vision lab as a lab assistant for a
										year and completed my master's with the thesis "Perceptual Grouping by Orientation Coherence" in September 2015. Next, 
										I moved to the Netherlands to pursue my Ph.D. under the supervision of prof. <a href="https://www.elkanakyurek.com/" target="_blank">Elkan Akyürek</a>  at the University of Groningen.
										I completed my Ph.D. with the dissertation titled: "From Stimulus to Representation" which investigated how stimuli influence
										temporal attention and perception.I continued working in the same lab for three years for my first postdoctoral project,
										working on NWO supported Open Research Area (ORA) project. We explored latent states of visual working memory using EEG. 
										After my first postdoc, I joined <a href="https://sites.google.com/site/fougnielab/home" target="_blank">Daryl Fougnie</a>'s 
										lab at New York University, Abu Dhabi campus. Our research topic covers attention and working memory functions.
										</div>  
                                </div>
                            </div>        
                        </div>
                    </div>
					<div class="pagecontents">
							<div class="section color-2">
							<div class="section-container">
							<div class="row">
									<div class="col-md-9 col-md-offset-1">
                                    <div class="title text-center">
                                        <h2>Interests</h2>
                                    </div>
									</div>
									<div class="col-md-5">
                                        <ul class="ul-boxed list-unstyled" data-columns="2">
                                            <li>Visual Attention</li>
                                            <li>Visual Perception</li>
                                            <li>Visual Working Memory</li>
										</ul>
									</div>
									<div class="col-md-5 col-md-offset-1">
                                        <ul class="ul-boxed list-unstyled" data-columns="2">
											<li>Temporal Integration</li>
                                            <li>Perceptual Grouping</li>
                                            <li>Cocoa Flavanols</li>
                                        </ul>
									</div>
							</div>
							</div>
							</div>
					</div>
                    <div class="pagecontents">
                        <div class="section color-1">
                            <div class="section-container">
                                <div class="row">
                                    <div class="col-md-5">
                                        <div class="title text-center">
                                            <h2>Education & Training</h2>
                                        </div>
                                        <ul class="ul-dates">
                                            <li>
                                                <div class="dates">
                                                    <span>Ph.D.</span>
                                                    <span>2020</span>
                                                </div>
                                                <div class="content">
                                                    <h4>Cognitive Neuroscience</h4>
                                                    <p><em>Rijksuniversiteit Groningen,</em> Groningen, the Netherlands</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dates">
                                                    <span>M.A.</span>
													<span>2015</span>
                                                </div>
                                                <div class="content">
                                                    <h4>Experimental Psychology</h4>
                                                    <p><em>Brooklyn College, City University of New York,</em> NY, the US</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dates">
                                                    <span>B.A.</span>
													<span>2011</span>
                                                </div>
                                                <div class="content">
                                                    <h4>Psychology</h4>
                                                    <p><em>Ankara University,</em> Ankara, Turkey</p>
                                                </div>
                                            </li>
                                            
                                        </ul>

                                    </div> 
									<div class="col-md-5 col-md-offset-1">
                                        <div class="title text-center">
                                            <h2>Academic Positions</h2>
                                        </div>
                                        <ul class="ul-dates">
											<li>
                                                <div class="dates">
                                                    <span>2022</span>
                                                    <span>2025</span>
                                                </div>
                                                <div class="content">
                                                    <h4>Postdoctoral Researcher</h4>
                                                    <p>New York University</p>
                                                </div>
                                            </li>
											<li>
                                                <div class="dates">
                                                    <span>2019</span>
                                                    <span>2022</span>
                                                </div>
                                                <div class="content">
                                                    <h4>Postdoctoral Researcher</h4>
                                                    <p>Rijksuniversiteit Groningen</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dates">
                                                    <span>2014</span>
                                                    <span>2015</span>
                                                </div>
                                                <div class="content">
                                                    <h4>Lab assistant</h4>
                                                    <p><em>Animal vision lab, </em>Brooklyn College, CUNY</p>
                                                </div>
                                            </li>
                                        </ul>
                                    </div>
                                </div>    
                            </div>
                                
                        </div>

                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h2>Honors, Awards and Grants</h2>
                                        </div>
                                        <ul class="timeline">
										
											<li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject">Open Research Award, Open Science Programme, University of Groningen</div>
                                                    <div class="text row">
                                                        <div class="col-md-2">
                                                            <img alt="image" class="thumbnail img-responsive" src="img/personal/OPA.jpeg" >
                                                        </div>
                                                        <div class="col-md-10">
                                                           Open Research award for the study ‘Concealed familiar face detection with pupillometry in rapid serial visual presentation’, by Open Science Programme, University of Groningen, Groningen, the Netherlands. 
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>            							
																				
											<li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject">Best poster presentation award, Working Memory Engram workshop</div>
                                                    <div class="text row">
                                                        <div class="col-md-2">
                                                            <img alt="image" class="thumbnail img-responsive" src="img/personal/WM_engram_icon.png" >
                                                        </div>
                                                        <div class="col-md-10">
                                                           Best poster presentation award at the Searching for the Working Memory Engram workshop held in University of Groningen, Groningen, the Netherlands. 
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>                                            
											
                                            <li class="open">
                                                <div class="date">2012-2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject">Scholarship from the Ministry of Education, Turkey (200.000$)</div>
                                                    <div class="text row">
                                                        <div class="col-md-2">
                                                            <img alt="image" class="thumbnail img-responsive" src="img/personal/awards100x100.png" >
                                                        </div>
                                                        <div class="col-md-10">
                                                           Scholarship covering both masters and PhD programs in the field of experimental psychology from the Ministry of Education, Turkey. 
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                         </ul>
                                    </div>
                                </div>
                            </div>
                        </div>                            
                    </div>
                </div>

                <div id="research" class="page">
					<div class="row2">
						<div id="rightlinks">
							<a href="indexTR.html">TR</a> 
							<a href="index.html">EN</a> 
						</div>	
					</div>
                    <div class="pageheader">
                        
                        <div class="headercontent">
							<div class="page-container">
								<div class="section-container">
									<h3 class="title">Blog</h3>
								</div>
								<div class="section color-2">
									<div class="section-container">
										<div class="row">
											<div class="col-md-12">
												<ul class="ul-withdetails">
													<div class="row">
                                                    <div class="col-sm-6 col-md-2">
                                                        <div class="image">
                                                            <img alt="image" src="img/blog/Psychonomics.jpeg" class="img-responsive2">
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-6 col-md-9">
                                                        <div class="meta">
                                                            <h2>Psychonomics Meeting (5/2018)</h2>
                                                            <p> International Psychonomics Meeting was held in Amsterdam which is close to Groningen, so that I took the chance and attended the conference with my veteran poster (Target color and contrast influences temporal attention...). It was really nice to attend such a well-organized conference. A couple of nights in the beautiful Amsterdam was relaxing as well. Overall, great experience! </p>
                                                        </div>
                                                    </div>
													</div>
												</ul>
											</div>
										</div>
                                    </div>
                                </div>
								<div class="section color-1">
									<div class="section-container">
										<div class="row">
											<div class="col-md-12">
												<ul class="ul-withdetails">
											<div class="row">
                                                    <div class="col-sm-6 col-md-2">
                                                        <div class="image">
                                                            <img alt="image" src="img/blog/darkchocolate.jpg" class="img-responsive2">
                                                        </div>
                                                    </div>
                                                    <div class="col-sm-6 col-md-9">
                                                        <div class="meta">
                                                            <h2>Mindwise-Blogpost (4/2018)</h2>
                                                            <p>I and <a href="https://www.elkanakyurek.com/" target="_blank">Elkan</a> wrote a blogpost about the flavanol study that published in Psychopharmacology on the<a href="http://mindwise-groningen.nl/"  target="_blank"> Mindwise </a>.  You can reach the full text via the <a href="http://mindwise-groningen.nl/attention-chocolate-can-give-you-an-edge/" target="_blank"> link</a>. Also, there is a <a href="https://www.reddit.com/r/Nootropics/comments/83i3kt/the_acute_effects_of_cocoa_flavanols_on_temporal/" target="_blank"> reddit post </a> about the study which I encountered; it was really nice to see that people are interested in our work. </p>
                                                        </div>
                                                    </div>
                                            </div>                                          
                                            
                                        </ul>
										</div>
										</div>
                                    </div>
                                </div>
								
								
							</div>
                        </div>    
                    </div>
                </div>
				
               <div id="publications" class="page">
                    <div class="row2">
						<div id="rightlinks">
							<a href="indexTR.html">TR</a> 
							<a href="index.html">EN</a> 
						</div>	
					</div>
					<div class="page-container">
                        <div class="pageheader">
                            <div class="headercontent">
                                <div class="section-container">
                                    <h2 class="title">Publications</h2>
                                </div>
                            </div>
                        </div>

                        <div class="pagecontents">
                            
                            <div class="section color-1" id="filters">
                                <div class="section-container">
                                    <div class="row">
                                        
                                        <div class="col-md-3">
                                            <h3>Filter by type:</h3>
                                        </div>
                                        <div class="col-md-6">
                                            <select id="cd-dropdown" name="cd-dropdown" class="cd-select">
                                                <option class="filter" value="all" selected>All types</option>
                                                <option class="filter" value="jpaper">Journal Papers</option>
                                                <option class="filter" value="cpaper">Conference Contributions</option>
                                                <!-- <option class="filter" value="bookchapter">Book Chapters</option>-->
                                                <option class="filter" value="book">Books</option>
                                                <!-- <option class="filter" value="report">Reports</option>
                                                <option class="filter" value="tpaper">Technical Papers</option> -->
                                            </select>
                                        </div>
                                        
                                        <div class="col-md-3" id="sort">
                                            <span>Sort by year:</span>
                                            <div class="btn-group pull-right"> 

                                                <button type="button" data-sort="data-year" data-order="desc" class="sort btn btn-default"><i class="icon-sort-by-order"></i></button>
                                                <button type="button" data-sort="data-year" data-order="asc" class="sort btn btn-default"><i class="icon-sort-by-order-alt"></i></button>
                                            </div>
                                        </div>    
                                    </div>
                                </div>
                            </div>

                            <div class="section color-2" id="pub-grid">
                                <div class="section-container">
                                    <div class="row">
                                        <div class="col-md-12">
                                            <div class="pitems">
												
												<div class="item mix jpaper" data-year="2024">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="https://escienceediting.org/journal/view.php?doi=10.6087/kcse.345" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
															<a href="https://escienceediting.org/upload/pdf/kcse-345.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<!--<a href="https://osf.io/wf5uv/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> -->
															
													    </div>
                                                        <h4 class="pubtitle">Our journey as intern junior editors at the journal of experimental psychology: Human perception and performance</h4>
                                                        <div class="pubauthor">Guérin, S.M.R., Tarlao, C., & <strong>Karabay, A.</strong></div>
                                                        <div class="pubcite"><span class="label label-success">Essay</span> Science Editing. (2024)</div>
                                                    </div>
                                                </div>															
												
												
												<div class="item mix jpaper" data-year="2024">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://link.springer.com/article/10.3758/s13428-024-02477-2#Sec33" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
															<a href="https://link.springer.com/content/pdf/10.3758/s13428-024-02477-2.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="https://osf.io/wf5uv/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
															
													    </div>
                                                        <h4 class="pubtitle">Introducing ART: a new method of testing auditory memory with circular reproduction tasks</h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>, Nijenkamp, R., Sarampalis, A., & Fougnie, D. </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Behavior Research Methods. (in press)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Theories of visual working memory have seen significant progress through the use of continuous reproduction tasks. However, these tasks have mainly focused on studying visual features, 
														with limited examples existing in the auditory domain. Therefore, it is unknown to what extent newly developed memory models reflect domain-general limitations or are specific to the visual
														domain. To address this gap, we developed a novel methodology: the Auditory Reproduction Task (ART). This task utilizes Shepard tones, which create an infinite rising or falling tone illusion
														by dissecting pitch chroma and height, to create a 1-360° auditory circular space. In Experiment 1, we validated the perceptual circularity and uniformity of this auditory stimulus space. 
														In Experiment 2, we demonstrated that auditory working memory shows similar set size effects to visual working memory—report error increased at set size two relative to one caused by swap errors.
														In Experiment 3, we tested the validity of ART by correlating reproduction errors with commonly used auditory and visual working memory tasks. Analyses revealed that ART errors were significantly
														correlated with performance in both auditory and visual working memory tasks, albeit with a stronger correlation observed with auditory working memory. While these experiments have only scratched
														the surface of the theoretical and computational constraints on auditory working memory, they provide a valuable proof-of-concept for ART. Further research with ART has the potential to deepen
														our understanding of auditory working memory, as well as to explore the extent to which existing models are tapping into domain-general constraints. 
														</p>
                                                    </div>
                                                </div>						
												
												
												<div class="item mix cpaper" data-year="2024">
													<div class="pubmain">
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle"> Evidence against levels of processing theories of visual awareness </h4>
                                                        <div class="pubauthor"> <strong>Karabay, A.</strong>, & Fougnie, D.  </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>  Asia Pacific Conference On Vision (APCV2024) in Singapore. (2024)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>There is debate about whether awareness during visual perception occurs abruptly (all-or-none) or gradually. One influential view is the
														levels of processing (LOP) theory which states that visual awareness depends on the stimulus. Low-level stimuli, such as color, evoke gradual
														awareness, while high-level stimuli, such as object identity, elicit abrupt, all-or-none perception. A critical source of evidence supporting
														LOP is that self-reported perceptual clarity measures reveal more intermediate values of perceptual clarity for low- (e.g. color) than high- 
														(e.g. letter) level stimuli. Here we provide several pieces of evidence inconsistent with this theory. First, previous studies confound stimulus
														level with category-flatness. Does increased perceptual clarity of X versus blue reflect that a noisy perception of X is perceived as X due to 
														large category priors for letter stimuli (e.g. there is no meaningful halfway point between X and M but there is between blue and red)? Consistent 
														with this, when we generated a high-level stimulus set that lacked meaningful category boundaries (by morphing unfamiliar faces into a continuous space)
														our results revealed gradual awareness. Second, by varying foil-target similarity, we show that an assumption underlying perceptual clarity measures—that
														they measure stimulus clarity not the difficulty of perceptual judgments—is incorrect. Finally, existing studies do not equate performance. This is 
														problematic because high-level stimuli have better performance, and the apparent all-or-none nature of categorical stimuli may reflect few intermediate 
														perceptual clarity ratings due to high confidence. Consistent with this, preliminary data suggests gradual awareness for all stimuli, including letters, 
														when task performance is equated across stimuli. Taken together, our findings reject the notion of separate awareness pathways for high- vs. low-level 
														stimuli and suggest that gradual versus all-or-none perception depends on other methodological properties.
														</p>
                                                    </div>
												</div>
												
												<div class="item mix cpaper" data-year="2024">
													<div class="pubmain">
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle"> Where’s Waldo in the mind: Accessing perceptual and semantic attributes in perception and working memory </h4>
                                                        <div class="pubauthor"> Sasin, E., Zhou, Y., <strong>Karabay, A.</strong>, Shrestha, S., & Fougnie, D.  </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>  The Vision Sciences Society Meeting (VSS2024) in Florida, USA. (2024)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>During perception, low-level features (such as color) are processed faster than high-level features (such as semantic properties). But what about
														accessing information from working memory? Recent work (Kong & Fougnie, 2021) has shown that search in working memory may be distinct from visual
														search regarding which features are most efficient. Further, research on long-term memory (Linde-Domingo, Treder, Kerrén, & Wimber, 2019) has 
														shown that semantic information is retrieved more rapidly than perceptual information. However, it is not yet known whether semantic properties
														are accessed faster from working memory than perceptual attributes. In two experiments, participants were shown four images that were either 
														animate or inanimate objects (semantic property) and which could be in the form of a photograph or drawing (perceptual property). Participants
														were pre-cued (perception – Experiment 1) or post-cued (working memory – Experiment 2) to the location of one of these objects. The cues were 
														accompanied by either a semantic (“animate or inanimate?”) or perceptual (“drawing or photograph?”) question. Unsurprisingly, perceptual aspects
														were discriminated faster than semantic aspects when the information was available to visual perception. However, when the task required accessing
														no longer presented information from working memory, participants took less time to respond to semantic than perceptual queries. These experiments,
														together with other recent findings, point to a reversal of the processing hierarchy for perception and memory. While visual perception is feed-forward,
														retrieving information in memory might first involve accessing high-level properties such as semantic categories, followed by access to lower-level visual properties.
														</p>
                                                    </div>
												</div>
												
												
												
												
												<div class="item mix jpaper" data-year="2024">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00173/120839/Concurrent-maintenance-of-both-veridical-and" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
															<a href="https://direct.mit.edu/imag/article-pdf/doi/10.1162/imag_a_00173/2373425/imag_a_00173.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="https://osf.io/3hdpc" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
															
													    </div>
                                                        <h4 class="pubtitle">Concurrent maintenance of both veridical and transformed working memory representations within unique coding schemes</h4>
                                                        <div class="pubauthor">Kandemir, G., Wolff, M.J., <strong>Karabay, A.</strong>, Stokes, M.G., Axmacher, N., & Akyürek, E.G.</div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Imaging Neuroscience. (2024)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>In the dynamic environment we live in, the already limited information that human working memory can maintain needs to be constantly updated
														to optimally guide behaviour. Indeed, previous studies showed that leading up to a response, representations maintained in working memory 
														representations are transformed continuously. This goes hand-in-hand with the removal of task-irrelevant items. However, does such removal
														also include the representations of stimuli as they were originally, prior to transformation? Here we assessed the neural representation of 
														task-relevant transformed representations, and the no-longer-relevant veridical representations they originated from. We applied multivariate
														pattern analysis to electroencephalographic data during maintenance of orientation gratings with and without mental rotation. During maintenance,
														we perturbed the representational network by means of a visual impulse stimulus, and were thus able to successfully decode veridical as well as
														imaginary, transformed orientation gratings from impulse-driven activity. The impulse response reflected only task-relevant (cued), 
														but not task-irrelevant (uncued) items, suggesting that the latter were quickly discarded from working memory. By contrast, even though the
														original cued orientation gratings were also no longer task-relevant after mental rotation, these items continued to be represented next to
														the rotated ones, in different representational formats. This seemingly inefficient use of scarce working memory capacity was associated with
														reduced probe response times and may thus serve to increase precision and flexibility in guiding behaviour in dynamic environments.
														</p>
                                                    </div>
                                                </div>						
												
												
												
												<div class="item mix jpaper" data-year="2024">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://www.sciencedirect.com/science/article/pii/S1053810023001642" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
															<a href="https://www.sciencedirect.com/science/article/pii/S1053810023001642/pdfft?md5=6013e46f651ec491aa01cdebbaed322b&pid=1-s2.0-S1053810023001642-main.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="https://osf.io/am5y7/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
															
													    </div>
                                                        <h4 class="pubtitle">Attentional blur and blink: Effects of adaptive attentional scaling on visual awareness</h4>
                                                        <div class="pubauthor">Wang, S., <strong>Karabay, A.</strong>, & Akyürek, E.G.</div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Consciusness and Cognition. (2024)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Attentional scaling is a crucial mechanism that enables us to flexibly allocate our attention to larger or smaller regions in 
														the visual field. Although previous studies have demonstrated the critical role of attentional scaling in visual processing, its 
														impact on modulating visual awareness is not yet fully understood. This study investigates the adaptive control of attentional scaling 
														and its influence on visual awareness in an attentional blink paradigm. Participants were required to attend to the first target’s location, 
														which was manipulated either session-wise, trial-wise, or such that it could be learned across a block of trials. Discrete, all-or-none, 
														awareness was expected when attention was allocated to a narrow area, while gradual awareness was expected when attention was allocated 
														to a larger area. We used mixture modeling to assess second target awareness across these different attentional scales. The results revealed 
														that participants could adaptively control their attentional scale both across stable sessions, and through (implicit) statistical learning 
														in blocks of successive trials. This produced gradual perceptual awareness when the participants adopted a broad attentional scale, causing 
														an attentional “blur”. However, trial-wise cues did not allow for attentional scaling, resulting in more discrete target perception overall, 
														and an attentional “blink”. We conclude that the attentional scale is to some extent under adaptive control during the attentional blink/blur, 
														where it can produce qualitatively different modes of perceptual awareness.
														</p>
                                                    </div>
                                                </div>						
												
												
												<div class="item mix cpaper" data-year="2023">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle"> Concealed familiar face detection with EEG in rapid serial visual presentation </h4>
                                                        <div class="pubauthor"> Chen, I., <strong>Karabay, A.</strong>, Mathȏt, S., Buchel, P., van der Mijn, R.,  Bowman, H., & Akyürek, E. G.  </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>  the 45th European Conference on Visual Perception (ECVP) in Paphos, Cyprus. (2023)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Classical concealed information tests (CITs) are in some circumstances able to detect concealed information, but are also vulnerable to countermeasures that participants can use to shield
															concealed information from detection. Rapid serial visual presentation (RSVP) has proven effective against such countermeasures, and can thus substantially reduce type-II error. Research to date has relied on classic univariate analyses of EEG
															data. Here we investigated whether RSVP-based CIT with multivariate analysis (decoding) of the EEG is potentially more effective for detecting ‘concealed knowledge’ of familiar faces. 29
															participants searched for a target face in an RSVP task while a familiar face (one of their parents’ faces), or one of two control
															faces also appeared in the stream. Using neural-network decoding, we detected concealed information for each individual
															with an average hit rate of 61.8% and an average correct rejection rate of 72.7%, while accuracy was 49.4% (around chance
															level), when we decoded one control face from the other. In comparison, univariate analyses were only able to detect familiar face recognition in 19 participants. Our findings suggest that 
															neural-network decoding makes RSVP-based CIT a more reliable method to detect concealed information.
														</p>
                                                    </div>
												</div>


												<div class="item mix cpaper" data-year="2023">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle"> Introducing ART: a new method of testing auditory memory with circular reproduction tasks </h4>
                                                        <div class="pubauthor"> Fougnie, D., <strong>Karabay, A.</strong>, Nijenkamp, R., & Sarampalis, A.  </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>  The Vision Sciences Society Meeting (VSS2023) in Florida, USA. (2023)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Working memory research has largely focused on the visual domain, however, a full picture of working memory requires understanding its properties 
														in other domains (e.g., auditory). Recently, working memory tasks have focused on circular reproduction tasks, since these allow the separation of 
														putatively distinct mechanisms. Critically, such tasks have been leveraged only to study visual features (e.g., color & orientation) and have not been 
														expanded to the auditory domain. Here we developed a new methodology: Auditory reproduction task (ART). To overcome the challenge of creating a circular 
														space with auditory information, we relied on Shepard tones which create an illusion of infinite rising or falling tone frequency. We converted an octave 
														range of Shepard tones to a 0-360° circular space. In the first experiment, we validated the perceptual circularity of the Shepard tones with multidimensional 
														scaling. The perceptual space of the tones resembled an almost perfect circle. In the second experiment, we demonstrated that auditory working memory shows 
														set-size effects, similar to visual working memory. Specifically, reproduction errors increased when participants were required to retain two (sequentially 
														presented) versus one tone. Further, the preliminary results, when subjected to mixture modeling, revealed that precision decreased as a function of set size, 
														demonstrating that the theoretical models on visual working memory can be used to study auditory working memory. Similar to visual working memory, we found 
														evidence that the non-tested item influenced responses to the tested item—repulsion when the distance between the two random tones was small and attraction 
														when the distance was large. Taken as a whole, these findings validate the ART task as a useful tool to study the properties of auditory working memory and 
														how they may be similar to (or differ from) visual working memory.
														</p>
                                                    </div>
												</div>
												
												
												<div class="item mix cpaper" data-year="2023">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle"> Concealed familiar face detection with oculomotor measures and EEG in rapid serial visual presentation </h4>
                                                        <div class="pubauthor"> Chen, I., Mathȏt, S., van der Mijn, R., <strong>Karabay, A.</strong>, Bowman, H., & Akyürek, E. G.  </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>  The Vision Sciences Society Meeting (VSS2023) in Florida, USA. (2023)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Traditional concealed information tests (CIT) work fairly well, but people can still use countermeasures to avoid detection. Presenting 
														the critical stimuli in rapid serial visual presentation (RSVP) while measuring EEG has proven effective against countermeasures. We investigated
														here to what extent concealed information (familiar faces) are also detectable in RSVP-based CIT with oculomotor measures, in particular pupil 
														size and micro-saccades. In our two studies (one with oculomotor measures and one with EEG), 31 and 34 participants, respectively, were asked to
														search for a target face in an RSVP task, while a familiar face, one of their parents’ faces, or a control face also appeared in the task. We 
														found that the pupil dilated more in response to the familiar faces, as compared to control faces, an effect that was most pronounced when looking 
														at the velocity of pupil-size changes, rather than pupil size itself. Micro-saccades did not seem to add much information. Overall, EEG remained 
														more sensitive than the oculomotor measures, but concealed information detection by means of the latter was nevertheless substantial. Taking 
														practical considerations into account, the application of oculomotor measures in RSVP-based CIT may thus present a viable alternative to EEG.
														</p>
                                                    </div>
												</div>


												<div class="item mix cpaper" data-year="2023">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Attentional blur and blink: Effects of adaptive attentional scaling on visual awareness </h4>
                                                        <div class="pubauthor">Wang, S., <strong>Karabay, A.</strong>, & Akyürek, E. G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span> 23st Conference of the European Society for Cognitive Psychology (ESCoP) in Porto, Portugal. (2023)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Attentional scaling is a mechanism allowing us to allocate our attention flexibly to larger or smaller regions, the role of which in visual processing has been established 
														in previous studies. However, little is known about its role in modulating visual awareness. This study investigated how attentional scaling can be adaptively controlled and 
														influence visual awareness. We employed an attentional blink (AB) paradigm that highlights the temporal limits of attentional selection. The first target’s location was either 
														explicitly cued, block-wise or trial-wise, or implicitly learned. Narrow attentional scaling resulted in discrete awareness, whereas broader one produced gradual awareness.
														Mixture modeling was used to assess second target awareness across attentional scaling conditions. We found that participants were able to adjust attentional scaling through
														both explicit block-wise cues and implicit learning, leading to a gradual awareness (attentional blur). Trialwise cues did not allow attentional scaling, causing more discrete
														target perception overall (attentional blink). Our study thus showed that attentional scaling could be adaptively controlled during the AB, leading to qualitatively different 
														perceptual awareness.
														</p>
                                                    </div>
												</div>												
												
												<div class="item mix cpaper" data-year="2023">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Concealed familiar face detection with EEG in rapid serial visual presentation </h4>
                                                        <div class="pubauthor">Chen, I., Mathȏt, S., van der Mijn, R., <strong>Karabay, A.</strong>, Bowman, H., & Akyürek, E. G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span> 23st Conference of the European Society for Cognitive Psychology (ESCoP) in Porto, Portugal. (2023)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Classical concealed information tests (CITs) are in some circumstances able to detect concealed information, but vulnerable to countermeasures 
														that participants can use to shield concealed information from detection. Rapid serial visual presentation (RSVP) has proven effective against such
														countermeasures, and can thus substantially reduce type-II error. Research to date has relied on classic univariate analyses of EEG data. Here we
														investigated whether RSVP-based CIT with multivariate analysis (decoding) of the EEG is potentially more effective for detecting 'concealed knowledge'
														of familiar face. 29 participants searched for a target face in an RSVP task while a familiar face (one of their parents' faces), or one of two control
														faces also appeared. Using neural network decoding, we detected concealed information for each individual with an average hit rate of 61.8% and an average 
														correct rejection rate of 72.7%, while accuracy was around chance level when we decoded one control face from the other. In comparison, univariate analyses
														were only able to detect familiar face recognition in 19 participants. Our findings suggest that neural network decoding makes RSVP-based CIT a more reliable
														method to detect concealed information.
														</p>
                                                    </div>
												</div>
												
												
												<div class="item mix jpaper" data-year="2023">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://journals.sagepub.com/doi/10.1177/02698811231161579" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
															<a href="https://journals.sagepub.com/doi/pdf/10.1177/02698811231161579?download=true" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="https://osf.io/tqyme/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
															
													    </div>
                                                        <h4 class="pubtitle">The effects of gamma-aminobutyric acid (GABA) on working memory and attention: A randomised, double-blind, placebo-controlled, crossover trial</h4>
                                                        <div class="pubauthor">Altınok, A., <strong>Karabay, A.</strong>, de Jonge, J., Balta, G., & Akyürek, E.G.</div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Journal of Psychopharmacology. (2023)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p><strong>Background:</strong> γ-aminobutyric acid (GABA) is a primary inhibitory neurotransmitter that plays a
														significant role in the central nervous system. Studies on both animals and humans show it has
														the pharmacological potential for reducing the impact of cognitive disorders, as well as enhancing
														cognitive functions and mood. However, its specific effects on human attention and working
														memory have not yet been extensively studied.</p>
														<p><strong>Aims:</strong> In this randomised, double-blind, placebo-controlled, and crossover trial, we aimed to test
														whether the administration of 800 mg GABA, dissolved in a drink, acutely affected visual working
														memory maintenance, as well as temporal and spatial attention in healthy adults.
														Methods: The participants were 32 young adults (16 females and 16 males). Working memory
														recall precision, spatial attention and temporal attention were measured by a delayed match-tosample task, a visual search task, and a speeded rapid serial visual presentation task,
														respectively. Participants completed two experimental sessions (GABA and Placebo) in
														randomized and counterbalanced order. In each session, forty-five minutes after administration
														of the drink, they completed the all three of the aforementioned cognitive tasks.</p>
														<p><strong>Results:</strong> Linear mixed model analysis results showed that GABA increased visual search time,
														compared to the placebo, but did not affect visual search accuracy, temporal attention, nor visual
														working memory precision.</p>
														<p><strong>Conclusions:</strong> The results suggest that GABA increases visual search time but does not affect
														temporal attention and memory, and that previously reported effects on cognition might rely on
														other functions.
														</p>
                                                    </div>
                                                </div>						
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Probing latent working memory representations in EEG </h4>
                                                        <div class="pubauthor">Akyürek, E. G., Kandemir, G., Wolff, M., <strong>Karabay, A.</strong> , & Wilhelm, S.  </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span> 22st Conference of the European Society for Cognitive Psychology (ESCoP) in Lille, France. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Working memory (WM) allows us to hold onto information for a brief, but critical interval, 
														thereby providing the backbone of intelligent, adaptive behavior. WM seems to rely not only on 
														ongoing neural activity, but also on connectivity, which is activity-quiescent or even 
														activity-silent. Measuring such latent states is challenging, since they are effectively 
														invisible to standard measures in cognitive neuroscience. However, by presenting a visual
														impulse to perturb the underlying brain network, in combination with multivariate pattern 
														analysis of the resultant impulse response signal, it is possible to illuminate and reveal
														representations held in quiescent network states. In a series of EEG experiments based on the
														perturbation technique, we found pervasive evidence for continued maintenance of seemingly
														useless information; from previously transformed items, to de-prioritized items, and 
														task-irrelevant properties. It thus seems that WM may hold more than meets the eye, particularly
														with regard to functionally and physiologically latent items.</p>
                                                    </div>
												</div>												
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle"> The Acute Effects of Caffeine and Cocoa Flavanols on Working Memory and Attention </h4>
                                                        <div class="pubauthor"> Altınok, A., <strong>Karabay, A.</strong>, Lorist, M. M., Weiden, D., & Akyürek, E.G.  </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span> 22st Conference of the European Society for Cognitive Psychology (ESCoP) in Lille, France. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>This study tests the acute synergistic effects of caffeine and cocoa flavanols (CF) consumption on working memory, spatial attention,
														and temporal attention in healthy adults with a randomized, double-blinded, placebo-controlled, counterbalanced, crossover, 
														and pre-registered design. While CF consumption facilitates attention due to vasodilation, caffeine improves information processing speed. 
														However, little is known about the synergetic effects of CF and caffeine on cognition. We will employ a free-recall task for working memory
														precision, a visual search task for spatial attention, and speeded rapid serial visual presentation task for temporal attention. In the 
														synergy condition, 200 mg caffeine and 415 mg CF will be administered prior to the test session, and subsequent task performance will be
														compared to a placebo condition. We expect caffeine and CF consumption to increase working memory precision and the accuracy of temporal 
														and spatial attention and potentially decrease reaction times in temporal and spatial attention tasks.
														</p>
                                                    </div>
												</div>
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Concealed familiar face detection with pupillometry in rapid serial visual presentation </h4>
                                                        <div class="pubauthor">Chen, I., Buchel, P.,<strong>Karabay, A.</strong>, Mathȏt, S., & Akyürek, E. G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span> 22st Conference of the European Society for Cognitive Psychology (ESCoP) in Lille, France. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Traditional concealed information tests (CIT) work fairly well, but people can still use countermeasures
														to avoid detection. Presenting the critical stimuli in rapid serial visual presentation (RSVP) while measuring
														EEG has proven effective against countermeasures. We investigated here whether pupil size is also an effective 
														measure in RSVP-based CIT. In our study, 31 participants were asked to search for a target face in an RSVP task
														while a familiar face, one of their parents' faces, and a control face also appeared in the task. We found that
														the pupil dilated more in response to the familiar faces, as compared to control faces. We also found that 7
														participants showed this effect when analysed individually. Our results show that an RSVP-based CIT with
														pupillometry can detect concealed familiar faces at a group level. Further development of the method may produce
														a valid and reliable concealed information detector at the individual level.
														</p>
                                                    </div>
												</div>
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle"> Pinging attentional task rules: Neural mechanisms underlying attending and suppressing in visual search tasks </h4>
                                                        <div class="pubauthor"> <strong>Karabay, A.</strong>, Kandemir, G., Kovács, E.R., & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span> 22st Conference of the European Society for Cognitive Psychology (ESCoP) in Lille, France. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Is attention a one-way street, or can we suppress stimuli the same as we attend to them? In a visual search task during
														which the direction of a target's tilt was reported, the nature of color cues was manipulated. Participants were informed
														that a color precue identified either the placeholder color of the target (attend), of a distractor (suppress), or neither
														of them (baseline). To measure latent EEG states, a visual impulse was inserted between the precue and visual search. 
														Analysis of reaction times showed a benefit in the attend condition compared to the suppress and baseline conditions.
														No benefit of the suppress condition over the baseline was observed. Drift diffusion models confirmed that the benefits
														in the attend condition were due to quicker non-decision time, indicating that attentional selection was faster. The EEG 
														patterns of colors will be contrasted to determine the task rule and infer underlying neural mechanisms.
														</p>
                                                    </div>
												</div>
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Which working memory model accounts best for target representation during the attentional blink? </h4>
                                                        <div class="pubauthor">Wang, S, <strong>Karabay, A.</strong>, & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span> 22st Conference of the European Society for Cognitive Psychology (ESCoP) in Lille, France. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>The attentional blink (AB) is a phenomenon in which identification of the second target is limited when it is presented
														shortly after the first one. One of the working memory (WM) models, the standard mixture model, has been widely used to
														investigate why such limitations arise. However, no existing study has systematically compared WM models in the AB domain.
														We compared eight commonly used visual WM models during the AB deficiency with data sets of three independent laboratories.
														Specifically, we utilized a maximum likelihood estimation to analyze these models and assessed them with the Bayesian 
														information criterion. Results suggest that the standard mixture model, slot model, and variants have better fitness to 
														most data sets than the swap, ensemble integration and variable precision models. Furthermore, stimuli characteristics 
														(e.g., colors or orientations) and their spatial arrangement in the AB task lead to different model rankings.
														</p>
                                                    </div>
												</div>
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">A visual impulse reveals memoranda embedded in functional connectivity: Evidence for activity-silent WM states </h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong> & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span>International Conference of Cognitive Neuroscience 2020 in Helsinki, Finland. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>A task-irrelevant, high-contrast stimulus can be used as a visual impulse signal to implement a functional
														non-invasive perturbation method that reveals working memory (WM) content. Stokes (2015) suggested
														that the impulse acts like a sonar signal used in echolocation, from which structural information (e.g., the
														surface of the ocean floor) can be derived. Similarly, in the brain the visual impulse reveals memoranda
														embedded in functional connectivity, which might by themselves be activity-silent. However, Barbosa et al.
														(2021) suggested that the impulse might only decrease non-WM-related EEG noise, thereby improving the
														ability to decode already-active memoranda. In this study, we sought to arbitrate between these two
														possibilities. We matched a task-irrelevant feature (spatial frequency) of a visual impulse with memory
														items (orientation gratings), while equalizing intensity and contrast. Better decoding of WM content in the
														match condition than in the no-match condition would suggest that the impulse interacts with the actual
														content within WM network, in line with activity-silent accounts. Conversely, if no differences between
														conditions are observed, this would fit with a noise reduction account, and suggest that WM might rely
														primarily on active storage. Results showed an advantage for matching impulses, supporting the former
														hypothesis that visual impulses work as a neural sonar. Further, although the visual impulse decreased
														average EEG variance, there was no difference between match and no-match conditions. We conclude that
														visual impulse perturbation reveals memoranda embedded in functional connectivity, in line with the idea
														that WM might rely on activity-silent states.</p>
                                                    </div>
												</div>	
											
											
												<div class="item mix jpaper" data-year="2023">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://onlinelibrary.wiley.com/doi/10.1111/psyp.14155" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
                                                            <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/psyp.14155" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															
															<a href="https://osf.io/9fkpm/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
													    </div>
                                                        <h4 class="pubtitle">Concealed identity information detection with pupillometry in rapid serial visual presentation</h4>
                                                        <div class="pubauthor">Chen, I. Y., <strong>Karabay, A.</strong> Mathot, S., Bowman, H., & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Psychophysiology. (2023)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>The concealed information test (CIT) relies on bodily reactions to stimuli that are hidden in mind. However, people can use countermeasures,
														such as purposely focusing on irrelevant things, to confound the CIT. A new method designed to prevent countermeasures uses rapid serial visual
														presentation (RSVP) to present stimuli on the fringe of awareness. Previous studies that used RSVP in combination with electroencephalography (EEG)
														showed that participants exhibit a clear reaction to their real first name, even when they try to prevent such a reaction (i.e. when their name is
														concealed information). Since EEG is not easily applicable outside the laboratory, we investigated here whether pupil size, which is easier to measure,
														can also be used to detect concealed identity information. In our first study, participants adopted a fake name, and searched for this name in an RSVP
														task, while their pupil sizes were recorded. Apart from this fake name, their real name and a control name also appeared in the task. We found pupil 
														dilation in response to the task-irrelevant real name, as compared to control names. However, while most participants showed this effect qualitatively,
														it was not statistically significant for most participants individually. In a second study, we preregistered the proof-of-concept methodology and 
														replicated the original findings. Taken together, our results show that the current RSVP task with pupillometry can detect concealed identity information
														at a group level. Further development of the method is needed to create a valid and reliable concealed identity information detector at the individual level.</p>
                                                    </div>
                                                </div>
												
																								
												<div class="item mix jpaper" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://psikolog.org.tr/en/publications/magazines/1031828/tpy1301996120211203m000043.pdf" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
													    </div>
                                                        <h4 class="pubtitle">The effects of long-term intake of cocoa flavanols on cognitive functions and mood, and the physiological mechanisms underlying these effects: A literature review</h4>
                                                        <div class="pubauthor">Karataş, O., <strong>Karabay, A.</strong>, & Alıcı, T.</div>
                                                        <div class="pubcite"><span class="label label-success">Review Paper</span> Turkish Psychological Articles. (2022)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>In recent years, the health benefits of flavanol-rich cocoa and cocoa-derived products, including on the nervous system, 
														have been clearly demonstrated. The purpose of this review is to summarize randomized controlled trials examining the effects 
														of long-term cocoa flavanols intake on mood and various cognitive functions, such as executive functions, attention, and memory,
														as well as to discuss mechanisms underlying these effects. In vivo and in vitro studies conducted with humans and experimental
														animals show that cocoa flavanols improve cognitive functions and mood by increasing the bioavailability of nitric oxide, which
														has a variety of functions, including dilating blood vessels, acting as a neurotransmitter, and improving insulin sensitivity.
														Furthermore, strong evidence has been presented that cocoa flavanols, which have a high antioxidant activity and neuroprotective
														properties, could support cognitive functions in cognitively intact individuals and prevent cognitive decline that inevitably
														occurs with aging through direct actions on receptors, enzymes, and signaling pathways. Despite the mixed findings observed in
														cocoa flavanols studies, long-term intake of cocoa flavanols, depending on the dose and duration of administration, regulate mood
														and support various cognitive functions, such as attention, processing speed, and working memory.</p>
                                                    </div>
                                                </div>
											
											
											
											
											
											<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">A visual impulse reveals memoranda embedded in functional connectivity: Evidence for activity-silent WM states </h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong> & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span>International Conference of Cognitive Neuroscience 2020 in Helsinki, Finland. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>A task-irrelevant, high-contrast stimulus can be used as a visual impulse signal to implement a functional
														non-invasive perturbation method that reveals working memory (WM) content. Stokes (2015) suggested
														that the impulse acts like a sonar signal used in echolocation, from which structural information (e.g., the
														surface of the ocean floor) can be derived. Similarly, in the brain the visual impulse reveals memoranda
														embedded in functional connectivity, which might by themselves be activity-silent. However, Barbosa et al.
														(2021) suggested that the impulse might only decrease non-WM-related EEG noise, thereby improving the
														ability to decode already-active memoranda. In this study, we sought to arbitrate between these two
														possibilities. We matched a task-irrelevant feature (spatial frequency) of a visual impulse with memory
														items (orientation gratings), while equalizing intensity and contrast. Better decoding of WM content in the
														match condition than in the no-match condition would suggest that the impulse interacts with the actual
														content within WM network, in line with activity-silent accounts. Conversely, if no differences between
														conditions are observed, this would fit with a noise reduction account, and suggest that WM might rely
														primarily on active storage. Results showed an advantage for matching impulses, supporting the former
														hypothesis that visual impulses work as a neural sonar. Further, although the visual impulse decreased
														average EEG variance, there was no difference between match and no-match conditions. We conclude that
														visual impulse perturbation reveals memoranda embedded in functional connectivity, in line with the idea
														that WM might rely on activity-silent states.</p>
                                                    </div>
												</div>	
											
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Concealed information detection with pupillometry in rapid serial visual presentation </h4>
                                                        <div class="pubauthor">Chen, I. Y., Buchel, P. <strong>Karabay, A.</strong> Mathot, S., Bowman, H., & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>International Conference of Cognitive Neuroscience 2020 in Helsinki, Finland. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>The concealed information test (CIT) relies on bodily reactions to stimuli that are hidden in mind.
														However, people can use countermeasures, such as purposely focusing on irrelevant things, to confound
														the CIT. A method designed to prevent the use of countermeasures, based on rapid serial visual
														presentation (RSVP), presents each stimulus on the fringe of awareness. Previous studies showed that this
														RSVP in combination with electroencephalography (EEG) is valid at detecting information with various levels
														of salience, even when participants try to prevent such a reaction. Since EEG measures are not easily
														applicable outside the laboratory, we investigated here whether pupil size, which is easier to measure, is
														also a valid measure with this RSVP-based CIT. In our first study, 31 participants were asked to adopt a fake
														name, and search for this name in an RSVP task, while their pupil sizes were recorded. Apart from this fake
														name, their real name and a control name also appeared in the task. We found that the pupil dilated more
														in response to the task-irrelevant real name, as compared to control names. However, while most
														participants showed this effect qualitatively, it was only statistically significant for 6 participants when
														analysed individually. As a second study, we preregistered the proof-of-concept methodology and
														replicated the findings of the first one. In our third study, 31 participants were asked to search for a target
														face in an RSVP task while one of their parents’ faces and a control face also appeared in the task. We found
														that the pupil dilated more in response to their parents’ face, as compared to control faces. We also found
														that 7 participants showed this effect when analysed individually. Taken together, our results show that the
														current RSVP task with pupillometry can detect concealed information of identity and parents’ faces at a
														group level. Further development of the method may produce a valid and reliable concealed information
														detector at the individual level.
														</p>
                                                    </div>
												</div>	
												
												<div class="item mix book" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://www.egitenkitap.com/fizyolojik-psikoloji?search=fizyolojik%20psikoloji" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">
                                                            Duyum ve Algı -  (Sensation and Perception)
                                                        </h4>
                                                        <div class="pubauthor"> <strong>Karabay, A.</strong></div>
                                                        <div class="pubcite">
                                                            <span class="label label-primary">Book Chapter</span>Fizyolojik Psikoloji. (2022)
                                                        </div>
                                                        
                                                    </div>
                                                </div>
											
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Which working memory model accounts best for target representation in the attentional blink?</h4>
                                                        <div class="pubauthor">Wang, S, <strong>Karabay, A.</strong>, & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>18th NVP Winter Conference in Egmond, Netherlands. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>People often fail to detect the second of two briefly presented target stimuli when the time interval between them is within 200-500 msec.
														This phenomenon is known as the attentional blink (AB). The current literature suggests that the standard mixture model of working memory (WM)
														explains representation during the AB better than resource-based models. However, no existing study has systematically compared WM models in 
														the AB domain. Here, we compare eight models commonly used in visual WM studies. Firstly, we fitted each model to the data collected from 3 
														separate laboratories. Next, the Bayesian information criterion (BIC) values were calculated for each model at an individual level, across 
														different conditions and experiments. Finally, the average model rankings were obtained based on the BIC values. Our findings indicated that,
														for most experiments presented here, the standard mixture model, the slot model, and their variants perform best in accounting for the data. 
														Meanwhile, the results also showed that the kind of stimuli (e.g., colors or orientations) and/or their spatial arrangement in the AB task can
														lead to markedly different model rankings. Our study demonstrates the applicability of WM models and allows for a principled selection of models
														in the AB field.
														</p>
                                                    </div>
												</div>	
												
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Concealed identity information detection with pupillometry in rapid serial visual presentation</h4>
                                                        <div class="pubauthor">Chen, I. Y., <strong>Karabay, A.</strong> Mathot, S., Bowman, H., & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>18th NVP Winter Conference in Egmond, Netherlands. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>The traditional concealed information test (CIT) can be confounded by countermeasures. Previous studies showed a new CIT method,
														based on rapid serial visual presentation (RSVP), in combination with electroencephalography (EEG), is able to prevent the use of 
														countermeasures when detecting concealed identity information (participants' real name). Since EEG measures are not easily applicable
														outside the laboratory, we investigated here whether pupil size, which is easier to measure, is also able to detect concealed identity
														information. In our study, 31 participants were asked to adopt a fake name, and search for this name in an RSVP task, while their pupil
														sizes were recorded. Apart from this fake name, their real name and a control name also appeared in the task. We found that the pupil
														dilated more in response to the task-irrelevant real name, as compared to control names. However, while most participants showed this
														effect qualitatively, it was not statistically significant for most participants when analysed individually. Taken together, our results
														show that the current RSVP task with pupillometry can detect concealed identity information at a group level. Further development of the
														method is needed to create a valid and reliable concealed identity information detector at the individual level.
														</p>
                                                    </div>
												</div>	
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Visual impulse perturbation: Neural sonar or just noise-reducer? </h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong> Wolff, M.J., Ruuskanen, V., & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>18th NVP Winter Conference in Egmond, Netherlands. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>A task-irrelevant, high contrast stimulus can be used as a visual impulse signal to implement a functional non-invasive 
														perturbation method that can reveal working memory (WM) content. Stokes (2015) suggested that the impulse acts like a sonar 
														signal used in echolocation, from which structural information (e.g., the surface of the ocean floor) can be derived. Similarly,
														in the brain the visual impulse reveals memoranda embedded in functional connectivity, which are by themselves activity-silent. 
														However, Barbosa et al. (2021) suggested that the impulse might only decrease non-WM-related EEG noise, thereby improving the
														ability to decode already-active memoranda. In this study, we sought to arbitrate between these two possibilities. We matched 
														a task-irrelevant feature (spatial frequency) of a visual impulse with the memory items (orientation gratings), while equalizing
														intensity and contrast. Better decoding of WM content in the match condition than in the no-match condition would suggest that 
														the impulse interacts with the actual content within WM network, in line with activity-silent accounts. Conversely, if no differences
														between conditions are observed, this would fit with a noise reduction account, and suggest that WM might rely primarily on active
														storage. Preliminary results showed an advantage for matching impulses, supporting the former hypothesis.
														</p>
                                                    </div>
												</div>	
												
												<div class="item mix cpaper" data-year="2022">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">The effects of gamma-aminobutyric acid (GABA) on working memory and attention</h4>
                                                        <div class="pubauthor">Altınok, A., Balta, G., <strong>Karabay, A.</strong>, & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>18th NVP Winter Conference in Egmond, Netherlands. (2022)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Our study aims to test the acute effects of GABA consumption on visual working memory, spatial attention and temporal attention in
														healthy adult volunteers. GABA acts as a neurotransmitter in the brain, and is also commonly found in various foods such as tomato, 
														sweet potato, spinach and soy beans. We hypothesize that GABA consumption might acutely affect cognitive processes. To test this idea,
														we set up a randomized, double-blinded, placebo-controlled, counterbalanced, and crossover experiment, with 32 young adults (aged 18-25)
														taking part. In the experiment, working memory recall precision will be measured in a task that requires the maintenance of grating 
														orientations in memory. Spatial attention will be measured with a visual search task, and a speeded rapid serial visual presentation 
														task will be used to measure temporal attention. In the critical condition, 800 mg GABA will be administered prior to the test session, 
														and subsequent task performance will be compared to a placebo condition. Participants’ body mass index and gender will also be considered
														in the analysis. We expect GABA consumption to increase working memory precision, as well as the accuracy of temporal and spatial attention,
														and potentially decrease reaction times in temporal and spatial attention tasks as well.
														</p>
                                                    </div>
												</div>	
												
												<div class="item mix jpaper" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://link.springer.com/article/10.1007/s00394-021-02767-x" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
															<a href="https://link.springer.com/content/pdf/10.1007/s00394-021-02767-x.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
													    </div>
                                                        <h4 class="pubtitle">Acute effects of cocoa flavanols on visual working memory: Maintenance and updating</h4>
                                                        <div class="pubauthor">Altinok, A., <strong>Karabay, A.</strong>, & Akyürek, E. G.  </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> European Journal of Nutrition. (2022)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p><strong>Background:</strong> Consumption of cocoa flavanols may have acute physiological effects on the brain due to their ability to activate nitric oxide synthesis. Nitric oxide mediates vasodilation, which increases cerebral blood flow, and can also act as a neurotransmitter.</p> 
														<p><strong>Objectives:</strong> This study aimed to examine whether cocoa flavanols have an acute influence on visual working memory (WM).</p> 
														<p><strong>Methods:</strong> Two randomised, double-blind, baseline- and placebo-controlled, counterbalanced crossover experiments were conducted on normal healthy young adult volunteers (NExp1=48 and NExp2=32, gender-balanced). In these experiments, 415 mg cocoa flavanols have been administered to show its acute effects on visual working memory. In the first experiment, memory recall precision was measured in a task that required only passive maintenance of grating orientations in WM. In the second experiment, recall was measured after active updating (mental rotation) of WM contents. Habitual daily flavanols intake, body mass index, and gender were also considered in the analysis.</p>
														<p><strong>Results:</strong> The results suggested that neither passive maintenance in visual WM nor active updating of WM was acutely enhanced by consumption of cocoa flavanols. Exploratory analyses with covariates (body mass index and daily flavanols intake), and the between-factor of gender also showed no evidence for effects of cocoa flavanols, neither in terms of reaction time, nor accuracy.</p>
														<p><strong>Conclusions:</strong> Overall, cocoa flavanols did not improve visual working memory recall performance during maintenance, nor did it improve recall accuracy after memory updating.</p>
                                                    </div>
                                                </div>
											<!--Articles in the format below-->
												<div class="item mix jpaper" data-year="2022">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://psycnet.apa.org/record/2022-05374-001" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
                                                            <a href="/Files/articles/Karabayetal2022JEPG.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="https://osf.io/x5dru/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
													    </div>
                                                        <h4 class="pubtitle">Two faces of perceptual awareness during the attentional blink: Gradual and discrete</h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>, Wilhelm, S. A., de Jonge, J., Wang, J., Martens, S., & Akyürek, E. G.  </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Journal of Experimental Psychology: General. (2022)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>In a series of experiments, the nature of perceptual awareness during the attentional blink was investigated. Previous 
														work has considered the attentional blink as a discrete, all-or-none phenomenon, indicative of general access to conscious
														awareness. Using continuous report measures in combination with mixture modeling, the outcomes showed that perceptual awareness
														during the attentional blink can be a gradual phenomenon. Awareness was not exclusively discrete, but also exhibited a gradual 
														characteristic whenever the spatial extent of attention induced by the first target spanned more than a single location. Under
														these circumstances, mental representations of blinked targets were impoverished, but did approach the actual identities of the 
														targets. Conversely, when the focus of attention covered only a single location, there was no evidence for any partial knowledge 
														of blinked targets. These two different faces of awareness during the attentional blink challenge current theories of both 
														awareness and temporal attention, which cannot explain the existence of gradual awareness of targets during the attentional blink.
														To account for the current outcomes, an adaptive gating model is proposed that casts awareness on a continuum between gradual and
														discrete, rather than as being of either single kind.</p>
                                                    </div>
                                                </div>
												
												<!--Book Chapter-->
												<div class="item mix book" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://pegem.net/urun/Herkes-Icin-Istatistiksel-Programlama-ve-Analiz/61909" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">
                                                            Herkes icin istatistiksel programlama ve analiz (Introduction to R)
                                                        </h4>
                                                        <div class="pubauthor">Richard Cotton, <i>translated to Turkish by</i> Sünbül, Ö., Sünbül, S.Ö., & <strong>Karabay, A.</strong></div>
                                                        <div class="pubcite">
                                                            <span class="label label-primary">Book translation</span> Pegem Academy Press, Istanbul, Turkey. (2020)
                                                        </div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                        <img alt="image" src="img/pubs/Rbook.png" align="left"  style="padding:0 30px 30px 0;">
                                                        <h4>Herkes icin istatistiksel programlama ve analiz</h4>
														<p>R istatistiksel programlama ve analiz için geliştirilmiş olan güçlü bir programlama dilidir. Yurtdışında yaygın olarak kullanılmakta ve birçok üniversitede lisansüstü ders olarak okutulmaktadır.  Bu programlama dili açık kaynak kodlu olduğundan dolayı programa rahatlıkla ulaşılabilmektedir ve program ücretsiz olarak kullanılabilmektedir. R istatistiksel analize ihtiyaç duyulan hemen hemen bütün bilim alanlarında (sosyal, sağlık, fen, ekonomi vb.) kullanılabilmektedir. R, çok geniş bir kütüphaneye sahiptir ve bu kütüphane gün geçtikçe artan bir ivmeyle gelişmeye devam etmektedir. R'ın yakın bir gelecekte popüler istatistiksel programların yerini alacağı düşünülmektedir. Kitabın içeriği basitten karmaşığa doğru olacak şekilde adım adım  kurgulanmıştır. Herhangi bir programlama dili deneyimine sahip olmayan bireylerin dahi rahatlıkla istatistiksel programlama ve analiz yapmasına olanak sağlamaktadır.</p>
                                                    </div>
                                                </div>
												
												<div class="item mix jpaper" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://www.journalofcognition.org/articles/10.5334/joc.127/" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i> 
                                                            </a>
                                                            <a href="https://www.journalofcognition.org/articles/10.5334/joc.127/galley/358/download/" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="https://osf.io/2dp6e/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
													    </div>
                                                        <h4 class="pubtitle">Discriminating global orientation of two element sets</h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong> &  Kurylo, D. D. </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Journal of Cognition. (2020)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Perceived global organization of visual patterns is based upon the aggregate contribution of constituent components. 
														Patterns constructed from multiple sources cooperate or compete for global organization. An investigation was made here 
														of interactions between two interspersed element sets on global orientation. It was hypothesized that each set would operate 
														as an integrated unit, and contribute independently to global orientation. Participants viewed a 10 x 10 array of Gabor patches,
														and indicated the predominant orientation of the array. In Experiment 1 8 all elements were rotated. Rotation up to 23° had little effect,
														whereas greater rotation produced a progressive shift on global orientation. In Experiment 2 a proportion of elements remained aligned
														while remaining elements were rotated. Embedding a proportion of aligned elements stabilized global orientation, which was dependent 
														upon the proportion of aligned elements. Specifically, with 20% alignment, global orientation was similar to rotating all elements,
														whereas 80% alignment strongly biased perception towards aligned elements. The stabilizing effect varied with rotation of the second 
														element set. Across levels of rotation, alignment effects rose to a peak then declined as element sets became orthogonal. 
														In Experiment 3, each element set was rotated independently. Independent rotation of both sets altered global orientation, 
														compressing the psychometric function for the single-element condition. Together, for interspersed element sets with explicit orientations,
														each set does not contribute independently to global orientation. Instead, element sets interact, where the contribution of one set, 
														presented at a fixed rotation and fixed proportion, varies with the change to the second set.</p>
                                                    </div>
                                                </div>
												
											<!--Books in the format below-->
												<div class="item mix book" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
															<a href="/Files/articles/FromStimulustoRepresentation_PhDthesis_book.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">
                                                            From stimulus to tepresentation: Target identification in rapid serial visual presentation
                                                        </h4>
                                                        <div class="pubauthor"> <strong>Karabay, A.</strong></div>
                                                        <div class="pubcite">
                                                            <span class="label label-primary">PhD thesis</span> (2020)
                                                        </div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                        <img alt="image" src="img/pubs/PhDthesis.png" align="left"  style="padding:0 30px 30px 0;">
                                                        <h4>From stimulus to tepresentation: Target identification in rapid serial visual presentation</h4>
														<p>This dissertation investigated the relationship between target identification and temporal integration with three studies. Rapid serial visual presentation tasks were used to investigate empirical questions, where two targets are embedded in a set of distracters, and the task is to identify targets. Temporal integration is a phenomenon that temporally separated targets fall into the same perceptual episode. The first study investigated whether a change in low-level stimulus features (color/contrast) influences temporal attention and integration. The results showed that a categorical change of target color decreases the competition between targets resulting in better target identification and more frequent integrations. Besides, the study confirmed the literature that high contrast stimuli mask low contrast stimuli if their temporal proximity is close. The second study investigated if Gestalt properties influence the temporal binding of targets and target identification. In addition to studies that show parts of objects are grouped in space, the second study showed that if temporally separated targets form a figure, their integrated percepts and identifications are facilitated. Lastly, how a change in mental state with consumption of cocoa flavanols, which increases blood flow in brain arteries, influences target identification and integrations were investigated. There was no effect of cocoa flavanols on temporal attention and integration. In addition, the study showed that cocoa flavanols improve the efficiency of visual search. In sum, target identification and integration are both influenced by exogenous stimuli properties and by flavanol-induced changes in mental state, and often also in a similar direction.</p>
                                                    </div>
                                                </div>
												
												<div class="item mix jpaper" data-year="2019">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://www.sciencedirect.com/science/article/pii/S0001691818305043" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i>
                                                            </a>
                                                            <a href="/Files/articles/TemporalIntegrationAttentionalSelection.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="https://osf.io/rwkx8/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
													    </div>
                                                        <h4 class="pubtitle">Temporal integration and attentional selection of color and contrast target pairs in rapid serial visual presentation</h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong> &  Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Acta Psychologica. (2019)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Performance in a dual target rapid serial visual presentation task was investigated, dependent on whether the
														color or the contrast of the targets was the same or different. Both identification accuracy on the second target,
														as a measure of temporal attention, and the frequency of temporal integration were measured. When targets had
														a different color (red or blue), overall identification accuracy of the second target and identification accuracy of
														the second target at Lag 1 were both higher than when targets had the same color. At the same time, increased
														temporal integration of the targets at Lag 1 was observed in the different color condition, even though actual
														(non-integrated) single targets never consisted of multiple colors. When the color pairs were made more similar,
														so that they all fell within the range of a single nominal hue (blue), these effects were not observed. Different
														findings were obtained when contrast was manipulated. Identification accuracy of the second target was higher
														in the same contrast condition than in the different contrast condition. Higher identification accuracy of both
														targets was furthermore observed when they were presented with high contrast, while target contrast did not
														influence temporal integration at all. Temporal attention and integration were thus influenced differently by target
														contrast pairing than by (categorical) color pairing. Categorically different color pairs, or more generally,
														categorical feature pairs, may thus afford a reduction in temporal competition between successive targets that
														eventually enhances attention and integration.</p>
                                                    </div>
                                                </div>
												
												<div class="item mix cpaper" data-year="2019">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
														<h4 class="pubtitle">The Attentional Blink: Binary Or Gradual?</h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>, Wang, J., Martens, S., & Akyürek, E. G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span>21st Conference of the European Society for Cognitive Psychology (ESCoP) in Tenerife, Spain. (2019)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Identification of the second of two targets (T2) is difficult
															when it follows the first one within 200-500 milliseconds.
															This so-called attentional blink (AB) may reflect that a
															missed T2 fails to reach post-perceptual processing. Alternatively,
															T2 may still reach working memory partially, or in
															a degraded fashion. To arbitrate between these possibilities,
															we applied mixture modeling to continuous target features
															(e.g., orientation). If T2 does not reach post-perceptual
															processing, responses should be random guesses, that
															is, uncorrelated with the target. If the T2 representation is
															only degraded, then errors should cluster around the target
															with a certain precision. We observed notable differences
															in AB tasks that are spatially variable and those that
															are not. In non-spatial tasks, T2 identification was binary;
															it either did or did not reach post-perceptual processing. In
															spatial tasks, however, T2 identification was graded, suggesting
															it was represented in working memory, but with
															decreased precision.
															</p>
                                                    </div>
												</div>	
												
												<div class="item mix cpaper" data-year="2019">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
														<h4 class="pubtitle">Decoding Sensory and Abstract Information From Activity Silent Brain States </h4>
                                                        <div class="pubauthor">Kandemir, G.,  <strong>Karabay, A.</strong>, & Akyürek, E. G.</div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span>21st Conference of the European Society for Cognitive Psychology (ESCoP) in Tenerife, Spain. (2019)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Distributed Working Memory (WM) models attribute different
															levels of WM representations to different regions of
															the brain. One highly debated factor is whether the information
															represented in different levels is retained via similar
															mechanisms. Recently it was shown that sensory information
															was maintained in activity-silent form and that the
															state of the network could be revealed following a perturbation
															by the presentation of a non-informative signal (impulse
															signal). We applied the same perturbation technique
															to representations in visual WM, which either corresponded
															to directly presented orientation gratings, or to
															stimuli that were recoded following abstract task rules that
															consisted of rotation instructions. The decoding of EEG recordings
															revealed that abstract task rules were also retained
															in activity-silent form and that the impulse signal
															boosted decoding accuracy during the activity-silent WM
															maintenance phase. Furthermore, the imagined orientations
															that were the product of the rotation task were also
															decodable from impulse-driven activity.</p>
                                                    </div>
												</div>	
												
												
												<div class="item mix cpaper" data-year="2019">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">The Acute Effects Of Cocoa Flavanols On Visual Working Memory </h4>
                                                        <div class="pubauthor">Altınok, A., <strong>Karabay, A.</strong>, & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>21st Conference of the European Society for Cognitive Psychology (ESCoP) in Tenerife, Spain. (2019)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Consumption of cocoa flavanols may have beneficial physiological
															effects on the brain due to their ability to activate
															nitric oxide synthesis. Nitric oxide mediates vasodilation,
															increasing cerebral blood flow, and can also act as a neurotransmitter.
															However, the cognitive consequences of cocoa
															flavanols remain underspecified. The aim of this study
															was to examine whether cocoa flavanols influence visual
															working memory (WM). We conducted two randomised,
															within-subjects, placebo controlled, double-blind experiments
															on normal healthy adult volunteers (N=48 and
															N=36, gender-balanced). In the first experiment, we measured
															passive maintenance of grating orientations in WM,
															whereas in the second experiment we measured active updating
															of WM (rotation). Precision and guess rates were
															analysed with MemToolBox. The results suggested that
															passive maintenance in visual WM is not enhanced by cocoa
															flavanols, possibly because it relies on activity-silent
															(synaptic) mechanisms. By contrast, preliminary results indicate
															that active updating of WM is affected by cocoa flavanols.</p>
                                                    </div>
												</div>	
												
												<div class="item mix cpaper" data-year="2019">
													<div class="pubmain">
													<!--<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>-->
														<h4 class="pubtitle">Decoding Visual Working Memory Before and After Mental Operations </h4>
                                                        <div class="pubauthor">Kandemir, G. <strong>Karabay, A.</strong>, & Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>BCN Winter Meeting at the University of Twente in Twente, Netherlands. (2019)</div>
													</div>
												</div>	
												
												
												
												
												
												
												<div class="item mix jpaper" data-year="2018">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
															</a>
                                                            <a href="https://link.springer.com/article/10.1007/s00213-018-4861-4" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i>
                                                            </a>
                                                            <a href="https://link.springer.com/content/pdf/10.1007%2Fs00213-018-4861-4.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
															<a href="Files/Data/CocoaFlavanolsAttention.zip" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a> 
															<a href="Files/Tasks/CocoaFlavanolsAttention.7z" class="tooltips" title="Task" target="_blank">
                                                                <i class="icon-cogs"></i>
                                                            </a> 
													    </div>
                                                        <h4 class="pubtitle">The acute effects of cocoa flavanols on temporal and spatial attention</h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>, Saija, J., Field, D., &  Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Psychopharmacology. (2018)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>In this study, we investigated how the acute physiological effects of cocoa flavanols might result in specific cognitive changes, in particular in temporal and spatial attention. To this end, we pre registered and implemented a randomized, double-blind, placeboand baseline-controlled crossover design. A sample of 48 university students participated in the study and each of them completed the experimental tasks in four conditions (baseline, placebo, low dose, and high-dose flavanol), administered in separate sessions with a 1-week washout interval. A rapid serial visual presentation task was used to test flavanol effects on temporal attention and integration, and a visual search task was similarly employed to investigate spatial attention. Results indicated that cocoa flavanols improved visual search efficiency, reflected by reduced reaction time. However, cocoa flavanols did not facilitate temporal attention nor integration, suggesting that flavanols may affect some aspects of attention, but not others. Potential underlying mechanisms are discussed.</p>
                                                    </div>
                                                </div>
												<div class="item mix jpaper" data-year="2017">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="https://link.springer.com/article/10.3758%2Fs13414-017-1333-6" class="tooltips" title="External link" target="_blank">
                                                                <i class="icon-external-link"></i>
                                                            </a>
                                                            <a href="https://rd.springer.com/content/pdf/10.3758%2Fs13414-017-1333-6.pdf" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
                                                        </div>
                                                        <h4 class="pubtitle">The effects of Kanizsa contours on temporal integration and attention in rapid serial visual presentation</h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong> &   Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-success">Journal Paper</span> Attention, Perception, & Psychophysics. (2017)</div>
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Performance in rapid serial visual presentation tasks has been shown to depend on the temporal integration of target stimuli when they are presented in direct succession. Temporal target integration produces a single, combined representation of visually compatible stimuli, which is comparatively easy to identify. It is currently unknown to what extent target compatibility affects this perceptual behavior, because it has not been studied systematically to date. In the present study, the effects of compatibility on temporal integration and attention were investigated by manipulating the Gestalt properties of target features. Of particular interest were configurations in which a global illusory shape was formed when all stimulus features were present; a Kanizsa stimulus, which was expected to have a unifying effect on the perception of the successive targets. The results showed that although the presence of a Kanizsa shape can indeed enhance temporal integration, this also was observed for other good Gestalts, such as due to common fate and closure. Identification accuracy seemed to vary, possibly as a result of masking strength, but this did not seem associated with attentional processing per se. Implications for theories of Gestalt processing and temporal integration are discussed.</p>
                                                    </div>
                                                </div>
										        
												<!--Conference contributions in the format below-->
												
												
												<div class="item mix cpaper" data-year="2017">
													<div class="pubmain">
														<h4 class="pubtitle">The Acute Effects of Cocoa Flavanols on Temporal and Spatial Attention </h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>, Saija J., David F., &  Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Talk</span>winter conference 2017 of De Nederlandse Vereniging voor Psychonomie in Egmond, Netherlands. (2017)</div>
													</div>
												</div>	
												
												
												<div class="item mix cpaper" data-year="2018">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                    </div>
														<h4 class="pubtitle">Target Color and Contrast Influences Temporal Attention in Rapid Serial Visual Presentations </h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>&  Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>20th Conference of the European Society for Cognitive Psychology (ESCoP) in Potsdam, Germany. & 40th European Conference on Visual Perception (ECVP) in Berlin, Germany. (2017) & Psychonomics International Meeting in Amsterdam. (2018) </div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Attentional blink (AB) is a phenomenon that identifying the second target (T2) stimulus is restricted when it follows the first target (T1) with a short interval (200-500 ms). Various factors modulate AB; in this study, we investigated how target (I) colors and (II) contrast influence temporal attention. Attentional blink/temporal integration task was adapted to study in order to test how different color/contrast pairs influence T2| T1 accuracy and temporal integration. There were two color/contrast conditions: single color/contrast (T1 and T2 colors/contrast matched), mixed color/contrast (T1 and T2 were different colors/contrast). (I) It is found that T2| T1 accuracy were higher in single color condition. Further color specific analysis showed that T1 and T2 accuracy was high when target color was red and T2| T1 identification was greater when T2 color was red. Moreover, greater integration was observed in mixed color condition. It is a surprising finding since targets did not contain multiple colors in any trials. (II) There was no difference between single and mixed contrast condition on T2| T1 accuracy and temporal integration. Greater T1 and T2 accuracy was observed when targets were in high contrast condition. Better T2| T1 identification was observed in the high contrast T2 condition. On the contrary, integration was affected by T2 contrast, and more integration was observed when T2 contrast was low. In conclusion, (I) temporal attention was influenced by target color-pair conditions; however (II) contrast condition does not influence temporal attention in the same way color-pairs does.</p>
                                                    </div>
												</div>	
												
												
												<div class="item mix cpaper" data-year="2017">
													<div class="pubmain">
													
														<h4 class="pubtitle">Kanizsa Effects on Temporal Integration and Attention </h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>&  Akyürek, E.G. </div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>Heymans symposium at the Rijksuniversiteit Groningen in Groningen, Netherlands. (2017)</div>
													</div>
												</div>	
												
												
												<div class="item mix cpaper" data-year="2015">
													<div class="pubmain">
													<div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>    
													</div>
														<h4 class="pubtitle">Perceptual Grouping by Orientation Coherence </h4>
                                                        <div class="pubauthor"><strong>Karabay, A.</strong>& Kurylo, D.</div>
                                                        <div class="pubcite"><span class="label label-warning">Poster</span>Science Day, Brooklyn College in NY, the US. (2015)</div>
													</div>
													<div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Perceptual grouping allows the unification of elements within complex visual scenes. Perceptual grouping can be based upon several relationships among stimulus elements, including common orientation. Grouping can be disrupted by introducing noise elements, which disengage binding among target elements. We investigated parameters that limit grouping by interferencefrom noise. Specifically, we determined (1) the level of deviation of noise elements and (2) proportion of noise-target elements required to break perceptual grouping.It was hypothesized that 45° deviation of noise elements is required before disruption is apparent, since 45°borders the level of rotation oriented towards an opposing grouping pattern. It was further hypothesized that stimuli exceeding 50% noise elements will disrupt perceptual grouping, since the dominant grouping pattern will be carried by noise elements. In order to test these hypotheses, we measured visual discrimination of visual patterns. Four subjects indicated the dominant grouping pattern (horizontal or vertical) of an array of Gabor patches (oriented gratings). Measurements were made at five levels of orientation coherence and six levels deviation of noise elements. Backward masking was used to limit processing time of the stimulus.Results showed that perceptual grouping decreased significantly (from 95% to 67%) in the 45° of deviation condition. With 60% noise elements,discrimination was reduced to chance (52%). These results indicate that noise serves to reverse perceived grouping when presented at magnitudes or proportions that dominate constituents of stimulus patterns.</p>
                                                    </div>
												</div>	
												
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>

                        </div>
                    </div>
                </div>


                <!--<div id="teaching" class="page">
                    <div class="pageheader">
                        <div class="headercontent">
                            <div class="section-container">
                                
                                <h2 class="title">Teaching</h2>
                                
                                <div class="row">
                                    <div class="col-md-12">
                                        <p>I like to teach, </p>                                                   
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="pagecontents">
                        <div class="section color-1">
                            <div class="section-container">
                                <div class="row">
                                    <div class="title text-center">
                                        <h3>Currrent Teaching</h3>
                                    </div>
                                    <ul class="ul-dates">
                                        <li>
                                            <div class="dates">
                                                <span>Present</span>
                                                <span>2017</span>
                                            </div>
                                            <div class="content">
                                                <h4>Honors Research Practicum</h4>
                                                <p>In this course, I and three honors student of the psychology (BA) students do a research project. The aim of the study is to show students how to do a research step by step. </p>
                                            </div>
                                        </li>
                                        <li>
                                            <div class="dates">
                                                <span>Present</span>
                                                <span>2003</span>
                                            </div>
                                            <div class="content">
                                                <h4>SELC 8160 Molar Endodontic Selective</h4>
                                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultrices ac elit sit amet porttitor. Suspendisse congue, erat vulputate pharetra mollis, est eros fermentum nibh, vitae rhoncus est arcu vitae elit.</p>
                                            </div>
                                        </li>
                                        <li>
                                            <div class="dates">
                                                <span>Present</span>
                                                <span>2010</span>
                                            </div>
                                            <div class="content">
                                                <h4>Endodontics Postdoctoral AEGD Program</h4>
                                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultrices ac elit sit amet porttitor. Suspendisse congue, erat vulputate pharetra mollis, est eros fermentum nibh, vitae rhoncus est arcu vitae elit.</p>
                                            </div>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div class="title text-center">
                                        <h3>Teaching History</h3>
                                    </div>
                                    <ul class="ul-dates-gray">
                                        <li>
                                            <div class="dates">
                                                <span>1997</span>
                                                <span>1995</span>
                                            </div>
                                            <div class="content">
                                                <h4>Preclinical Endodnotics</h4>
                                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultrices ac elit sit amet porttitor. Suspendisse congue, erat vulputate pharetra mollis, est eros fermentum nibh, vitae rhoncus est arcu vitae elit.</p>
                                            </div>
                                        </li>
                                        <li>
                                            <div class="dates">
                                                <span>2005</span>
                                                <span>2003</span>
                                            </div>
                                            <div class="content">
                                                <h4>SELC 8160 Molar Endodontic Selective</h4>
                                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultrices ac elit sit amet porttitor. Suspendisse congue, erat vulputate pharetra mollis, est eros fermentum nibh, vitae rhoncus est arcu vitae elit.</p>
                                            </div>
                                        </li>
                                        <li>
                                            <div class="dates">
                                                <span>2011</span>
                                                <span>2010</span>
                                            </div>
                                            <div class="content">
                                                <h4>Endodontics Postdoctoral AEGD Program</h4>
                                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultrices ac elit sit amet porttitor. Suspendisse congue, erat vulputate pharetra mollis, est eros fermentum nibh, vitae rhoncus est arcu vitae elit.</p>
                                            </div>
                                        </li>
                                        <li>
                                            <div class="dates">
                                                <span>2011</span>
                                                <span>2010</span>
                                            </div>
                                            <div class="content">
                                                <h4>Endodontics Postdoctoral AEGD Program</h4>
                                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultrices ac elit sit amet porttitor. Suspendisse congue, erat vulputate pharetra mollis, est eros fermentum nibh, vitae rhoncus est arcu vitae elit.</p>
                                            </div>
                                        </li>
                                        <li>
                                            <div class="dates">
                                                <span>2011</span>
                                                <span>2010</span>
                                            </div>
                                            <div class="content">
                                                <h4>Endodontics Postdoctoral AEGD Program</h4>
                                                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ultrices ac elit sit amet porttitor. Suspendisse congue, erat vulputate pharetra mollis, est eros fermentum nibh, vitae rhoncus est arcu vitae elit.</p>
                                            </div>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>-->
                
                <!--<div id="gallery" class="page">
                    <div class="pagecontents">
                        
                        <div class="section color-3" id="gallery-header">
                            <div class="section-container">
                                <div class="row">
                                    <div class="col-md-3">
                                        <h2>Gallery</h2>
                                    </div>
                                    <div class="col-md-9">
                                        <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="section color-3" id="gallery-large">
                            <div class="section-container">
                                
                                <ul id="grid" class="grid">
                                    <li>
                                        <div>
                                            <img alt="image" src="img/gallery/450x600.png">
                                            <a href="img/gallery/450x600.png" class="popup-with-move-anim">
                                                <div class="over">
                                                    <div class="comein">
                                                        <i class="icon-search"></i>
                                                        <div class="comein-bg"></div>
                                                    </div>
                                                </div>
                                            </a>
                                        </div>
                                    </li>
                                    <li>
                                        <div>
                                            <img alt="image" src="img/gallery/600x600.png">
                                            <a href="img/gallery/600x500.png" class="popup-with-move-anim">
                                                <div class="over">
                                                    <div class="comein">
                                                        <h3>Image Title</h3>
                                                        <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                                                        <div class="comein-bg"></div>
                                                    </div>
                                                </div>
                                            </a>
                                        </div>
                                    </li>
                                    <li>
                                        <div>
                                            <img alt="image" src="img/gallery/900x600.png">
                                            <a href="img/gallery/900x600.png" class="popup-with-move-anim">
                                                <div class="over">
                                                    <div class="comein">
                                                        <i class="icon-search"></i>
                                                        <div class="comein-bg"></div>
                                                    </div>
                                                </div>
                                            </a>
                                        </div>
                                    </li>
                                    <li>
                                        <div>
                                            <img alt="image" src="img/gallery/400x300.png">
                                            <a href="img/gallery/400x300.png" class="popup-with-move-anim"> 
                                                <div class="over">
                                                    <div class="comein">
                                                        <h3>Image Title</h3>
                                                        <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                                                        <div class="comein-bg"></div>
                                                    </div>
                                                </div>
                                            </a>
                                        </div>
                                    </li>
                                    <li>
                                        <div>
                                            <img alt="image" src="img/gallery/400x300.png">
                                            <a href="img/gallery/400x300.png" class="popup-with-move-anim">
                                                <div class="over">
                                                    <div class="comein">
                                                        <i class="icon-search"></i>
                                                        <div class="comein-bg"></div>
                                                    </div>
                                                </div>
                                            </a>
                                        </div>
                                    </li>
                                    <li>
                                        <div>
                                            <img alt="image" src="img/gallery/800x600.png">
                                            <a href="img/gallery/800x600.png" class="popup-with-move-anim">
                                                <div class="over">
                                                    <div class="comein">
                                                        <h3>Image Title</h3>
                                                        <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                                                        <div class="comein-bg"></div>
                                                    </div>
                                                </div>
                                            </a>
                                        </div>
                                    </li>
                                    
                                    
                                </ul>
                                    
                            </div>
                        </div>
                    </div>
                    
                </div>-->
				<div id="experiments" class="page">
					<div class="row2">
						<div id="rightlinks">
							<a href="indexTR.html">TR</a> 
							<a href="index.html">EN</a> 
						</div>	
					</div>
                    <div class="pageheader">
                        
                        <div class="headercontent">
							<div class="page-container">
								<div class="section-container">
									<h3 class="title">Tasks</h3>
								</div>
							
							
							
                            <div class="section color-2" id="pub-grid">
                                <div class="section-container">
                                    <div class="row">
                                        <div class="col-md-12">
                                            <div class="pitems">
												
											<!--Tasks in the format below-->
												<div class="item mix jpaper" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="/Files/Tasks/VisualSearchTask.osexp" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
														
															<!--<a href="https://osf.io/2dp6e/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a>-->
													    </div>
														<img alt="image" src="img/experiments/VS.gif" align="left" style="padding:0 20px 20px 0;" width=250, height = 220>
														    <h3><b>Serial visual search task </b></h3>
                                                            <p>Visual search tasks are a common way that people look for specific objects or features in complex visual
															environments. Serial visual search tasks, in particular, refer to a type of visual search that involves searching
															through a visual display in a sequential or linear manner until a target object or feature is found. In a serial
															visual search task, participants typically start at a specific location in the visual display and move their attention
															in a systematic way to scan the rest of the display until they find the target. Serial visual search tasks can be influenced by a 
															number of factors, including the number of items in the display, the similarity netween the target and distractor items.
															</p>
													</div>
                                                    
                                                    <div class="pubdetails">
														<h4><b>Task Details</b></h4> 
                                                        <p> This task is used in Altinok et al. (2023). You can find all necessary details on the manuscript.
														The VS task had 30 practice and 300 experimental trials (100 trials for each condition) with 10 blocks. Each block had 30 trials,
														and participants were allowed to have a break between the blocks. The first trial of each block started when the participant pressed 
														the spacebar. The fixation dot was then shown for 300–500ms, followed by the search array, which was presented for 1000ms and covered
														by a mask for the next 1000ms. The search array always contained a single target letter, and a variable number of distractors, either 14, 20
														or 26. Participants were instructed to report the orientation of the target letter (T) as quickly and accurately as possible. 
														The participants had until the offset of the mask, that is, a total of 2000 ms, to give their response. Responses were given
														with the arrow keys on the keyboard. After the response, participants received feedback for 175ms: Either a happy or an unhappy
														smiley depending on their accuracy. The next trial started following an intertrial interval of 250–300 ms after the offset of the
														feedback display.
														</p>
														<p><u>Independent variables:</u> Number of distractors, 14,20,26</p>
														<p><u>Dependent Variable:</u> Accuracy, response time</p>
														
														<p><i>Feel free to use it, but don`t forget to cite the papers below</i></p>
														<img alt="image" src="img/experiments/VS.png" width=500 >
														<p></p>
														<p></p>
														<p></p>
														<h4><b>References</b></h4>
														<p>Altınok, A., Karabay, A., Balta, G, de Jonge, J., & Akyürek, E.G. (2023). The effects of gamma-aminobutyric
														acid (GABA) on working memory and attention: A randomised, double-blind, placebo-controlled, crossover
														trial. </i>Journal of Psychopharmacology. </i> doi: 10.1177/02698811231161579
														</p>
														
														<p>Mathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences.
														<i>Behavior Research Methods, 44</i>(2), 314-324. doi:10.3758/s13428-011-0168-</p>

                                                    </div>
                                              
												</div> 
												
												<div class="item mix jpaper" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="/Files/Tasks/hybridRSVP.osexp" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
														
															<!--<a href="https://osf.io/2dp6e/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a>-->
													    </div>
														<img alt="image" src="img/experiments/AB-integration.gif" align="left" style="padding:0 20px 20px 0;" width=250>
														    <h3><b>Hybrid RSVP -<i>Temporal Integration & Attentional Blink Task</i></b></h3>
                                                            <p> Attentional blink (AB) is a phenomenon associatiated with a decreased identification accuracy of the second target
															when it follows the first one between 200 to 500 ms (Raymond, Shapiro, & Arnell, 1992). AB is typically tested with rapid serial
															visual presentation (RSVP) tasks that consist of targets and distractors. When targets follow each other without
															intervening distractors, targets may fall into same perceptual episode if they are compatible. Temporal integration (Hommel &
															Akyurek, 2005) is a phenomenon which can be observed in RSVP tasks. With this hybrid RSVP task, both temporal
															integration frequency and AB magnitude can be measured with a single task. 
															</p>
													</div>
                                                    
                                                    <div class="pubdetails">
														<h4><b>Task Details</b></h4> 
                                                        <p> A variant of this hybrid RSVP task was used to test the effects of target feature change on temporal target identification
														(Karabay & Akyurek, 2019). This task can be used to test attentional performance. There are total of 7 possible targets, and 
														on each trial one or two targets are shown to the participants. The task is to report targets in correct temporal order at the
														end of the trial. Task details can be found in (Karabay & Akyurek, 2019). The difference of this task and the referred task
														is that, targets are only blue in this one.	There are	24 practice and 340 experimental trials and 
														it takes 45 minutes to complete. <a href="https://osdoc.cogsci.nl/3.3/download/" target="_blank">Open Sesame</a> 
														is used to create the experiment.
														You can stick target identities on numpad. A png file is added in the file pool showing corresponding keyboard buttons for
														each target type.

														</p>
														<p><u>Independent variables:</u> Lag, 1,3,8</p>
														<p><u>Dependent Variable:</u> T1 accuracy, T2|T1 Accuracy, Temporal Integration, Order Reversals</p>
														
														<p><i>Feel free to use it, but don`t forget to cite the papers below</i></p>
														<img alt="image" src="img/experiments/DualRSVP.png" width=500 >
														<p></p>
														<p></p>
														<p></p>
														<h4><b>References</b></h4>
														<p>
														Karabay, A. & Akyürek, E. G. (2019). Temporal integration and attentional selection of color
														and contrast target pairs in rapid serial visual presentation. <i>Acta Psychologica, 196</i>, 56–69.
														doi:10.1016/j.actpsy.2019.04.002</p>
														<p>
														Akyürek, E. G., Eshuis, S. A. H., Nieuwenstein, M. R., Saija, J. D., Başkent, D., & Hommel, B. (2012). Temporal target integration 
														underlies performance at Lag 1 in the attentional blink. <i>Journal of Experimental Psychology: Human Perception and Performance, 
														38</i>(6), 1448–1464. doi:10.1037/a0027610
														</p>
														<p>
														Raymond, J. E., Shapiro, K. L., & Arnell, K. M. (1992). Temporary suppression of visual processing in an 
														RSVP task: An attentional blink? <i>Journal of Experimental Psychology: Human Perception and Performance, 18</i>, 
														849–860. doi:10.1037/0096-1523.18.3.849
														</p>
														<p>
														Hommel, B., & Akyurek, E. G. (2005). Lag-1 sparing in the attentional blink: Benefits and costs of integrating two
														events into a single episode. <i>The Quarterly Journal of Experimental Psychology, 58A</i>(8), 1415–1433. 
														doi:10.1080/02724980443000647										
														</p>														
														<p>Mathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences.
														<i>Behavior Research Methods, 44</i>(2), 314-324. doi:10.3758/s13428-011-0168-</p>
														
                                                    </div>
                                              
												</div> 
												
												<div class="item mix jpaper" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="/Files/Tasks/DT_CLAB.osexp" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
														
															<!--<a href="https://osf.io/2dp6e/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a>-->
													    </div>
														<img alt="image" src="img/experiments/DTCLAB.gif" align="left" style="padding:0 20px 20px 0;" width=250>
														    <h3><b>Dwell Time Paradigm</b></h3>
                                                            <p> The dwell Time paradigm (DT) is pioneered by Duncan, Ward, and Shapiro (1994). In this task, how attention
															dwells from one location to another over time is tested. Like an attentional blink, target identification performance
															is impaired when targets follow each other for about half a second. We adapted the color reproduction task to the DT paradigm
															to apply working memory models on DT (Karabay et al., in press)
															</p>
													</div>
                                                    
                                                    <div class="pubdetails">
														<h4><b>Task Details</b></h4> 
                                                        <p> This task is Experiment 2B of the Karabay et al. (in press). We used this task to assess the nature of visual awareness. 
														    This task can be used to estimate attentional dwell time, and model the information processing using WM models. Two colors 
															are chosen from CIELAB color space and shown to participants. Participants are asked to reproduce target color with a 
															mouse in the end of trial. The task takes 60-75 minutes to complete. <a href="https://osdoc.cogsci.nl/3.3/download/" target="_blank">Open Sesame</a> 
															is used to create the experiment. 
														<p>
														<i> Note: </i> This task requires colour and colormath libraries of OpenSesame. If you would like to use this task, open Opensesame with 
														administrator rights and run the code below in the debug window. 
														<p></p>
														<code>
														<p>import pip</p>
														<p>pip.main(['install', 'colormath'])</p>
														<p>pip.main(['install', 'colour'])</p>
														</p>
														</p>
														</code>
														<p><u>Independent variables:</u> SOA, 250 ms, 800 ms</p>
														<p><u>Dependent Variable:</u> Target 1 Reproduction error, Target 2 Reproduction error</p>
														
														<p><i>Feel free to use it, but don`t forget to cite the papers below</i></p>
														<img alt="image" src="img/experiments/DTCLAB.png"  >
														<p></p>
														<p></p>
														<p></p>
														<h4><b>References</b></h4>
														<p>
														Karabay, A., Wilhelm, S. A., de Jonge, J., Wang, J., Martens, S., & Akyürek, E. G. (in press). Two faces of
														perceptual awareness during the attentional blink: Gradual and discrete. <i>Journal of Experimental
														psychology: General</i>. doi:10.1037/xge0001156</p>
														<p>Duncan, J., Ward, R., & Shapiro, K. (1994). <i>Direct measurement of attentional dwell time in human vision. Nature, 369</i>(6478), 313–315. doi:10.1038/369313a0</p>

														<p>Mathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences.
														<i>Behavior Research Methods, 44</i>(2), 314-324. doi:10.3758/s13428-011-0168-</p>
														
                                                    </div>
                                              
												</div>
												<!--Tasks in the format below-->

												<div class="item mix jpaper" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="/Files/Tasks/VWM.osexp" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
														
															<!--<a href="https://osf.io/2dp6e/" class="tooltips" title="Data" target="_blank">
                                                                <i class="icon-bar-chart"></i>
                                                            </a>-->
													    </div>
														<img alt="image" src="img/experiments/WM_task.gif" align="left" style="padding:0 20px 20px 0;" width=250>
														    <h3><b>Visual Working Memory - <i>Orientation Reproduction Task</i></b></h3>
                                                            <p> Working memory (WM) is the memory that information is not only retained but also manipulated in short term. There are various
															conceptual, neural, computational theories explaining how WM works. The orientation reproduction task is a very modern
															WM task that the reproduction error can be measured precisely. The strength of reproduction tasks is that the quality of WM
															can be measured in a continuous manner rather than binary. Plus, reproduction errors can be modeled as guess rate and precision.
															</p>
													</div>
                                                    
                                                    <div class="pubdetails">
														<h4><b>Task Details</b></h4> 
                                                        <p> A varient of this visual working memory (VWM) task was used to test effects of cocoa flavanols on VWM. 
															You can find details on the manuscript (Altinok, Karabay,& Akyurek, in prep). This task can 
															be used to estimate VWM capacity, VWM performance, precision...etc. After a brief period of fixation dot,
															a memory array appears on the screen for 250 ms. Depending on the 
															load condition 1 to 4 items appear 100 pixel far from the center. Participants are asked to remember all orientations.
															After a second of retention interval, participants are asked to reproduce one of the memory item randomly. There are 
															16 practice and 240 experimental trials and it takes 20 - 30 minutes to complete. <a href="https://osdoc.cogsci.nl/3.3/download/" target="_blank">Open Sesame</a> 
															is used to create the experiment. 
														</p>
														<p><u>Independent variables:</u> WM load conditions, 1,2,3,4</p>
														<p><u>Dependent Variable:</u> Reproduction error</p>
														
														<p><i>Feel free to use it, but don`t forget to cite the papers below</i></p>
														<img alt="image" src="img/experiments/WMtask.png"  >
														<p></p>
														<p></p>
														<p></p>
														<h4><b>References</b></h4>
														<p>
														Altinok, A., Karabay, A., & Akyurek, E. G. (2020). Acute effects of cocoa flavanols on visual working memory: No evidence from two randomised, 
														double-blind, baseline- and placebo-controlled, crossover trials. <i>[under review]</i>.</p> 
														
														
														<p>Mathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences.
														<i>Behavior Research Methods, 44</i>(2), 314-324. doi:10.3758/s13428-011-0168-</p>
														
                                                    </div>
                                              
												</div> 
												 
											<!--Tasks in the format below-->	
											<!--
												<div class="item mix jpaper" data-year="2020">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            <a href="#" class="pubcollapse">
                                                                <i class="icon-expand-alt"></i>
                                                            </a>
                                                            <a href="/Files/Tasks/Stroop.osexp" class="tooltips" title="Download" target="_blank">
                                                                <i class="icon-cloud-download"></i>
                                                            </a>
													
													    </div>
														<img alt="image" src="img/experiments/Stroop_task.gif" align="left" style="padding:0 20px 20px 0;" width=250 height=210>
														    <h3><b>Stroop Task</b></h3>
                                                            <p> Stroop effect is a well-known psychological effect/phenomenon that was discovered by John Ridley Stroop in 1935.
															There are thousands of articles that use the Stroop task to answer their research question of interest. It is a 
															well-established task that measures selective attention capacity, and processing speed. Participants are asked to 
															report the ink color of the color name as quickly as possible. Shorter reaction times are observed when the text 
															and ink colors match (congruent condition) compared to when they mismatch (incongruent condition).
															</p>
													</div>
                                                    
                                                    <div class="pubdetails">
														<h4><b>Task Details</b></h4> 
                                                        <p> Participants are asked to report ink colors of the color names as quickly as possible. Responses 
														are	collected via keyboard.There are eight blocks and each block consist of 16 trials. 
														There are four possible colors: Red, blue, green and yellow. For instance;
														</p>
														<p><b>Congruent Condition</b></p>
														<p> <b style="color:red;"> RED </b>  ~ You need to press 'r' because the text color is red.</p>
														<p> <b style="color:blue;"> BLUE </b> ~ You need to press 'b' because the text color is blue.</p>
														<p> <b style="color:#fff700;"> YELLOW </b> ~ You need to press 'y' because the text color is yellow.</p>
														<p> <b style="color:green;"> GREEN </b> ~ You need to press 'g' because the text color is green.</p>
														<p><b>Incongruent Condition</b></p>
														<p> <b style="color:green;"> RED </b> ~ You need to press 'g' because the text color is green.</p>
														<p> <b style="color:red;"> BLUE </b> ~ You need to press 'r' because the text color is red.</p>
														<p> <b style="color:blue;"> YELLOW </b> ~ You need to press 'b' because the text color is blue.</p>
														<p> <b style="color:#fff700;"> GREEN </b> ~ You need to press 'y' because the text color is yellow.</p>
														<p><u>Independent variables:</u> Congruency: Congruent, Incongruent</p>
														<p><u>Dependent Variable:</u> Accuracy, Reaction Time</p>
														
														<p><i>Feel free to use it, but don`t forget to cite the papers below</i></p>
														<img alt="image" src="img/experiments/Strooptask.png"  >
														<p></p>
														<p></p>
														<p></p>
														<h4><b>References</b></h4>
														<p>
														Stroop, J. R. (1935). Studies of interference in serial verbal reactions. <i>Journal of Experimental Psychology, 18</i>
														(6), 643–662. doi:10.1037/h0054651
														</p> 
														
														<p>Mathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences.
														<i>Behavior Research Methods, 44</i>(2), 314-324. doi:10.3758/s13428-011-0168-</p>
														
														</p>
                                                    </div>
                                              
												</div> 
												
												
												-->
													
												
												</div>		
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>

							

						</div>		
					</div>
                      
					
                </div>
                <div id="contact" class="page stellar">
					<div class="row2">
						<div id="rightlinks">
							<a href="indexTR.html">TR</a> 
							<a href="index.html">EN</a> 
						</div>	
					</div>
                    <div class="pageheader">
                        <div class="headercontent">
                            <div class="section-container">
                                
                                <h2 class="title">Contact & Collaborate</h2>
                            
                                <div class="row">
                                    <div class="col-md-8">
                                        <p>Please do not hesitate to contact if you have any questions about my research, or if you would like to collaborate. </p>                              
                                    </div>
                                    <div class="col-md-4">
                                        <ul class="list-unstyled">
                                            <li>
                                                <strong><i class="icon-phone"></i>&nbsp;&nbsp;</strong>
                                                <span>office: +31 50 36 36657</span>
                                            </li>
                                            <!--<li>
                                                <strong><i class="icon-phone"></i>&nbsp;&nbsp;</strong>
                                                <span>lab: 808-808 88 88</span>
                                            </li>-->
                                            <li>
                                                <strong><i class="icon-envelope"></i>&nbsp;&nbsp;</strong>
                                                <span>a. karabay@ nyu.edu (delete the spaces)</span>
                                            </li>
                                            <!--<li>
                                                <strong><i class="icon-google-plus"></i>&nbsp;&nbsp;</strong>
                                                <span>jdoe@gmail.com</span>
                                            </li>-->
                                            <li>
                                                <strong><i class="icon-skype"></i>&nbsp;&nbsp;</strong>
                                                <span>aytac kara bay (delete the spaces)</span>
                                            </li>
                                            <li>
                                                <strong><i class="icon-twitter"></i>&nbsp;&nbsp;</strong>
                                                <span><a href="https://twitter.com/aytckrby" target="_blank">aytckrby</a></span>
                                            </li>
                                            <li>
                                                <strong><i class="icon-linkedin-sign"></i>&nbsp;&nbsp;</strong>
                                                <span><a href="https://www.linkedin.com/in/ayta%C3%A7-karabay-306252a2" target="_blank">aytackarabay</a></span>
                                            </li>
                                        </ul>    

                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                </div>
                
                <div id="overlay"></div>
            
            </div>
        </div>
		<!-- Default Statcounter code for personal website
		http://www.aytackarabay.com -->
		<script type="text/javascript">
		var sc_project=11668669; 
		var sc_invisible=1; 
		var sc_security="ff612015"; 
		</script>
		<script type="text/javascript"
		src="https://www.statcounter.com/counter/counter.js"
		async></script>
		<noscript><div class="statcounter"><a title="Web Analytics
		Made Easy - StatCounter" href="http://statcounter.com/"
		target="_blank"><img class="statcounter"
		src="//c.statcounter.com/11668669/0/ff612015/1/" alt="Web
		Analytics Made Easy - StatCounter"></a></div></noscript>
		<!-- End of Statcounter Code -->
		
    </body>
</html>

